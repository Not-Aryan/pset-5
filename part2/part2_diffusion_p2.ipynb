{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42ab1764",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'part2_diffusion_p2.ipynb\\n\\nAutomatically generated by Colaboratory.\\n\\nOriginal file is located at\\n    https://colab.research.google.com/drive/1... # Placeholder, adjust if needed\\n\\n# Acknowledgement\\n\\nParts of this pset were inspired by\\n* Berkeley CS294-158, taught by Pieter Abbeel, Wilson Yan, Kevin Frans, and Philipp Wu;\\n* MIT 6.S184/6.S975, taught by Peter Holderrieth and Ezra Erives;\\n* The [blog post](https://lilianweng.github.io/posts/2021-07-11-diffusion-models/) about diffusion models by Lilian Weng.\\n\\n\\n# Submission Guideline for Part 2\\n\\nPlease include your answer to all problems, including formulas, proofs, and the figures generated in each problem, excluding code. You are required to submit the (single) pdf and all (four) notebooks (one for each problem) with your code and running outputs. Do not include code in the pdf file.\\n\\nSpecifically, for Problem 2 in this notebook, the pdf should contain:\\n- The generated figures `results/p2_train_plot.png` and `results/p2_toy_samples.png`\\n\\n# Problem 2: Training Diffusion Models on a Toy Dataset\\nIn this problem, we will write the code for training and sampling from a diffusion model on a 2D toy dataset. This part requires GPUs--you can use Google Colab for GPU access. To work on this notebook in Google Colab, copy the `pset-5` directory to your Google Drive and open this notebook. Then, start working on a GPU machine with `Runtime -> Change runtime type -> T4 GPU`.\\n\\n## Data Generation and Visualization\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"part2_diffusion_p2.ipynb\n",
    "\n",
    "Automatically generated by Colaboratory.\n",
    "\n",
    "Original file is located at\n",
    "    https://colab.research.google.com/drive/1... # Placeholder, adjust if needed\n",
    "\n",
    "# Acknowledgement\n",
    "\n",
    "Parts of this pset were inspired by\n",
    "* Berkeley CS294-158, taught by Pieter Abbeel, Wilson Yan, Kevin Frans, and Philipp Wu;\n",
    "* MIT 6.S184/6.S975, taught by Peter Holderrieth and Ezra Erives;\n",
    "* The [blog post](https://lilianweng.github.io/posts/2021-07-11-diffusion-models/) about diffusion models by Lilian Weng.\n",
    "\n",
    "\n",
    "# Submission Guideline for Part 2\n",
    "\n",
    "Please include your answer to all problems, including formulas, proofs, and the figures generated in each problem, excluding code. You are required to submit the (single) pdf and all (four) notebooks (one for each problem) with your code and running outputs. Do not include code in the pdf file.\n",
    "\n",
    "Specifically, for Problem 2 in this notebook, the pdf should contain:\n",
    "- The generated figures `results/p2_train_plot.png` and `results/p2_toy_samples.png`\n",
    "\n",
    "# Problem 2: Training Diffusion Models on a Toy Dataset\n",
    "In this problem, we will write the code for training and sampling from a diffusion model on a 2D toy dataset. This part requires GPUs--you can use Google Colab for GPU access. To work on this notebook in Google Colab, copy the `pset-5` directory to your Google Drive and open this notebook. Then, start working on a GPU machine with `Runtime -> Change runtime type -> T4 GPU`.\n",
    "\n",
    "## Data Generation and Visualization\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55c76481",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_s_curve\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from os.path import exists, dirname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02457cf8",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1d98d39",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def toy_2d_data(n=100000):\n",
    "    x, _ = make_s_curve(n, noise=0.1)\n",
    "    x = x[:, [0, 2]]\n",
    "    return x.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7fe31fce",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def visualize_toy_2d_dataset():\n",
    "    data = toy_2d_data()\n",
    "    plt.figure()\n",
    "    plt.scatter(data[:, 0], data[:, 1])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b0e16e",
   "metadata": {},
   "source": [
    "visualize_toy_2d_dataset() # Commented out for script execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0cf6ef36",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\s'\n",
      "/var/folders/qx/4sngfkkj5kv2y_sh8nnhj1yr0000gn/T/ipykernel_12721/2168152498.py:1: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n## Training and Sampling of Diffusion Models\\n\\nFor code simplicity, we will train a continuous-time variant of the diffusion prompt. In practice training objectives and code between discrete-time and continuous-time diffusion models are similar.\\n\\nGiven a data element $x$ and neural net $f_\\theta(x, t)$, implement the following diffusion training steps:\\n\\n0. Construct a class `Diffusion`\\n1. Sample the diffusion timestep: $t \\\\sim \\text{Uniform}(0, 1)$\\n2. Compute the noise-strength following a cosine schedule: $\\x07lpha_t = \\\\cos\\\\left(\\x0crac{\\\\pi}{2}t\\right), \\\\sigma_t = \\\\sin\\\\left(\\x0crac{\\\\pi}{2}t\\right)$\\n3. Sample noise $\\\\epsilon \\\\sim N(0,I)$ (same shape as $x$) and cmpute noised $x_t = \\x07lpha_t x + \\\\sigma_t \\\\epsilon$\\n4. Estimate $\\\\hat{\\\\epsilon} = f_\\theta(x_t, t)$\\n5. Optimize the loss $L = \\\\lVert \\\\epsilon - \\\\hat{\\\\epsilon} \\rVert_2^2$. Here, it suffices to just take the mean over all dimensions.\\n\\nNote that for the case of continuous-time diffusion, the forward process is $x_{0\\to1}$ and reverse process is $x_{1\\to0}$\\n\\nUse an MLP for $f_\\theta$ to optimize the loss. You may find the following details helpful.\\n* Normalize the data using mean and std computed from the train dataset\\n* Train 100 epochs, batch size 1024, Adam with LR 1e-3 (100 warmup steps, cosine decay to 0)\\n* MLP with 4 hidden layers and hidden size 64\\n* Condition on t by concatenating it with input x (i.e. 2D x + 1D t = 3D cat(x, t))\\n* Training 100 epochs takes about 2 minutes on the Google Colab T4 GPU.\\n\\n\\nTo sample, implement the standard DDPM sampler. You may find the equation from the [DDIM paper](https://arxiv.org/pdf/2010.02502.pdf) helpful, rewritten and re-formatted here for convenience.\\n$$x_{t-1} = \\x07lpha_{t-1}\\\\left(\\x0crac{x_t - \\\\sigma_t\\\\hat{\\\\epsilon}}{\\x07lpha_t}\\right) + \\\\sqrt{\\\\sigma_{t-1}^2 - \\\\eta_t^2}\\\\hat{\\\\epsilon} + \\\\eta_t\\\\epsilon_t$$\\nwhere $\\\\epsilon_t \\\\sim N(0, I)$ is random Gaussian noise. For DDPM, let\\n$$\\\\eta_t = \\\\sigma_{t-1}/\\\\sigma_t\\\\sqrt{1 - \\x07lpha_t^2/\\x07lpha_{t-1}^2}$$\\n*Note*: As a sanity check, when $\\\\eta_t$ follows the DDPM setting as shown above, the resulted coefficient of $x_t$ and $\\\\hat{\\\\epsilon}$ should be the same as $A'$ and $B'$ you derived in Problem 1.2 (6) where $\\x07lpha_t$ here corresponds to $\\x08ar{\\x07lpha_t}$ in Problem 1.\\n\\nTo run the reverse process, start from $x_1 \\\\sim N(0, I)$. Then perform `num_steps` DDPM updates (a hyperparameter), pseudocode below.\\n```\\nts = linspace(1 - 1e-4, 1e-4, num_steps + 1)\\nx = sample_normal\\nfor i in range(num_steps):\\n    t = ts[i]\\n    tm1 = ts[i + 1]\\n    eps_hat = model(x, t)\\n    x = DDPM_UPDATE(x, eps_hat, t, tm1)\\nreturn x\\n```\\n*Note*:\\n* If you encounter NaNs, you may need to clip $\\\\sigma_{t-1}^2 - \\\\eta_t^2$ to 0 if it goes negative, as machine precision issues can make it a very small negative number (e.g. -1e-12) if its too close to 0\\n* For debugging, you can start with trying small number of epochs. To debug your training code, you can check whether the training and testing losses decrease properly. To debug your sampling code, you can check whether the generated distribution is close to the S-shape target distribution with large `num_steps`.\\n* To check your answer, the final text loss is roughly around 0.4.\\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "## Training and Sampling of Diffusion Models\n",
    "\n",
    "For code simplicity, we will train a continuous-time variant of the diffusion prompt. In practice training objectives and code between discrete-time and continuous-time diffusion models are similar.\n",
    "\n",
    "Given a data element $x$ and neural net $f_\\theta(x, t)$, implement the following diffusion training steps:\n",
    "\n",
    "0. Construct a class `Diffusion`\n",
    "1. Sample the diffusion timestep: $t \\sim \\text{Uniform}(0, 1)$\n",
    "2. Compute the noise-strength following a cosine schedule: $\\alpha_t = \\cos\\left(\\frac{\\pi}{2}t\\right), \\sigma_t = \\sin\\left(\\frac{\\pi}{2}t\\right)$\n",
    "3. Sample noise $\\epsilon \\sim N(0,I)$ (same shape as $x$) and cmpute noised $x_t = \\alpha_t x + \\sigma_t \\epsilon$\n",
    "4. Estimate $\\hat{\\epsilon} = f_\\theta(x_t, t)$\n",
    "5. Optimize the loss $L = \\lVert \\epsilon - \\hat{\\epsilon} \\rVert_2^2$. Here, it suffices to just take the mean over all dimensions.\n",
    "\n",
    "Note that for the case of continuous-time diffusion, the forward process is $x_{0\\to1}$ and reverse process is $x_{1\\to0}$\n",
    "\n",
    "Use an MLP for $f_\\theta$ to optimize the loss. You may find the following details helpful.\n",
    "* Normalize the data using mean and std computed from the train dataset\n",
    "* Train 100 epochs, batch size 1024, Adam with LR 1e-3 (100 warmup steps, cosine decay to 0)\n",
    "* MLP with 4 hidden layers and hidden size 64\n",
    "* Condition on t by concatenating it with input x (i.e. 2D x + 1D t = 3D cat(x, t))\n",
    "* Training 100 epochs takes about 2 minutes on the Google Colab T4 GPU.\n",
    "\n",
    "\n",
    "To sample, implement the standard DDPM sampler. You may find the equation from the [DDIM paper](https://arxiv.org/pdf/2010.02502.pdf) helpful, rewritten and re-formatted here for convenience.\n",
    "$$x_{t-1} = \\alpha_{t-1}\\left(\\frac{x_t - \\sigma_t\\hat{\\epsilon}}{\\alpha_t}\\right) + \\sqrt{\\sigma_{t-1}^2 - \\eta_t^2}\\hat{\\epsilon} + \\eta_t\\epsilon_t$$\n",
    "where $\\epsilon_t \\sim N(0, I)$ is random Gaussian noise. For DDPM, let\n",
    "$$\\eta_t = \\sigma_{t-1}/\\sigma_t\\sqrt{1 - \\alpha_t^2/\\alpha_{t-1}^2}$$\n",
    "*Note*: As a sanity check, when $\\eta_t$ follows the DDPM setting as shown above, the resulted coefficient of $x_t$ and $\\hat{\\epsilon}$ should be the same as $A'$ and $B'$ you derived in Problem 1.2 (6) where $\\alpha_t$ here corresponds to $\\bar{\\alpha_t}$ in Problem 1.\n",
    "\n",
    "To run the reverse process, start from $x_1 \\sim N(0, I)$. Then perform `num_steps` DDPM updates (a hyperparameter), pseudocode below.\n",
    "```\n",
    "ts = linspace(1 - 1e-4, 1e-4, num_steps + 1)\n",
    "x = sample_normal\n",
    "for i in range(num_steps):\n",
    "    t = ts[i]\n",
    "    tm1 = ts[i + 1]\n",
    "    eps_hat = model(x, t)\n",
    "    x = DDPM_UPDATE(x, eps_hat, t, tm1)\n",
    "return x\n",
    "```\n",
    "*Note*:\n",
    "* If you encounter NaNs, you may need to clip $\\sigma_{t-1}^2 - \\eta_t^2$ to 0 if it goes negative, as machine precision issues can make it a very small negative number (e.g. -1e-12) if its too close to 0\n",
    "* For debugging, you can start with trying small number of epochs. To debug your training code, you can check whether the training and testing losses decrease properly. To debug your sampling code, you can check whether the generated distribution is close to the S-shape target distribution with large `num_steps`.\n",
    "* To check your answer, the final text loss is roughly around 0.4.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43881b0d",
   "metadata": {},
   "source": [
    "--- Diffusion Model Components ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7baea9a7",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def alpha_sigma_schedule(t):\n",
    "    \"\"\"Cosine schedule for alpha_t and sigma_t.\"\"\"\n",
    "    alpha_t = torch.cos(math.pi / 2 * t)\n",
    "    sigma_t = torch.sin(math.pi / 2 * t)\n",
    "    return alpha_t, sigma_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6585078a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Diffusion:\n",
    "    def __init__(self, model, data_shape):\n",
    "        \"\"\"\n",
    "        model: neural network to estimate eps_hat (MLP in this problem)\n",
    "        data_shape: size of the input data, (2,) in this case\n",
    "        \"\"\"\n",
    "        self.model = model.to(DEVICE)\n",
    "        self.data_shape = data_shape\n",
    "\n",
    "    def loss(self, x):\n",
    "        \"\"\"\n",
    "        x: the input data (without adding noise) from the dataloader\n",
    "        Return:\n",
    "            The loss (as a scalar averaged over all data in the batch)\n",
    "        \"\"\"\n",
    "        # 1. Sample t ~ Uniform(0, 1)\n",
    "        t = torch.rand(x.shape[0], device=DEVICE) # Sample one t per data point in the batch\n",
    "\n",
    "        # 2. Compute alpha_t, sigma_t\n",
    "        alpha_t, sigma_t = alpha_sigma_schedule(t)\n",
    "        alpha_t = alpha_t.view(-1, *([1] * (x.dim() - 1))) # Reshape for broadcasting\n",
    "        sigma_t = sigma_t.view(-1, *([1] * (x.dim() - 1)))\n",
    "\n",
    "        # 3. Sample epsilon ~ N(0, I) and compute x_t\n",
    "        epsilon = torch.randn_like(x, device=DEVICE)\n",
    "        x_t = alpha_t * x + sigma_t * epsilon\n",
    "\n",
    "        # 4. Estimate eps_hat\n",
    "        eps_hat = self.model(x_t, t) # Pass t directly\n",
    "\n",
    "        # 5. Compute loss L = ||epsilon - eps_hat||^2 (mean over batch and dims)\n",
    "        loss = torch.mean((epsilon - eps_hat)**2)\n",
    "        return loss\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sample(self, n, num_steps):\n",
    "        \"\"\"\n",
    "        n: number of samples to generate\n",
    "        num_steps: number of steps in the diffusion sampling\n",
    "        Return:\n",
    "            The generated sample. Tensor with shape (n, *self.data_shape)\n",
    "        \"\"\"\n",
    "        # Define timesteps\n",
    "        ts = torch.linspace(1.0 - 1e-4, 1e-4, num_steps + 1, device=DEVICE)\n",
    "\n",
    "        # Start with x1 ~ N(0, I)\n",
    "        x = torch.randn(n, *self.data_shape, device=DEVICE)\n",
    "\n",
    "        self.model.eval() # Ensure model is in eval mode\n",
    "\n",
    "        # DDPM sampling loop\n",
    "        for i in range(num_steps):\n",
    "            t_ = ts[i]\n",
    "            tm1_ = ts[i + 1]\n",
    "\n",
    "            # Prepare t and tm1 tensors for the model and schedule\n",
    "            t = torch.full((n,), t_, device=DEVICE)\n",
    "            tm1 = torch.full((n,), tm1_, device=DEVICE)\n",
    "\n",
    "            # Get alpha and sigma for t and t-1\n",
    "            alpha_t, sigma_t = alpha_sigma_schedule(t)\n",
    "            alpha_tm1, sigma_tm1 = alpha_sigma_schedule(tm1)\n",
    "\n",
    "            # Reshape for broadcasting\n",
    "            alpha_t = alpha_t.view(-1, *([1] * (x.dim() - 1)))\n",
    "            sigma_t = sigma_t.view(-1, *([1] * (x.dim() - 1)))\n",
    "            alpha_tm1 = alpha_tm1.view(-1, *([1] * (x.dim() - 1)))\n",
    "            sigma_tm1 = sigma_tm1.view(-1, *([1] * (x.dim() - 1)))\n",
    "\n",
    "            # Predict epsilon_hat\n",
    "            eps_hat = self.model(x, t_) # Model expects scalar t per batch item? No, pass t tensor\n",
    "\n",
    "            # Calculate eta_t for DDPM\n",
    "            # eta_t = sigma_{t-1}/sigma_t * sqrt(1 - alpha_t^2/alpha_{t-1}^2)\n",
    "            # Avoid division by zero if alpha_tm1 is close to 0 (i.e., tm1 is close to 1)\n",
    "            ratio_alpha_sq = (alpha_t / alpha_tm1)**2\n",
    "            # Clip ratio_alpha_sq to prevent issues near t=1 where alpha_tm1 -> 0\n",
    "            ratio_alpha_sq = torch.clamp(ratio_alpha_sq, max=1.0 - 1e-6) # Ensure sqrt arg is non-negative\n",
    "            eta_t_sq = (sigma_tm1 / sigma_t)**2 * (1 - ratio_alpha_sq)\n",
    "            eta_t = torch.sqrt(torch.clamp(eta_t_sq, min=1e-8)) # Clip for sqrt and avoid division by zero if sigma_t is small\n",
    "\n",
    "            # Calculate term for epsilon_hat coefficient: sqrt(sigma_{t-1}^2 - eta_t^2)\n",
    "            # sigma_tm1_sq_minus_eta_t_sq = sigma_tm1**2 - eta_t**2 # This can be negative due to precision\n",
    "            # From derivation, sqrt(sigma_{t-1}^2 - eta_t^2) = sqrt(sigma_{t-1}^2 * (1 - (sigma_{t-1}^2 / sigma_t^2 * (1 - alpha_t^2 / alpha_{t-1}^2)) / sigma_{t-1}^2))\n",
    "            # simplifies to sigma_{t-1} * sqrt(1 - (sigma_{t-1}^2 / sigma_t^2 * (1 - alpha_t^2 / alpha_{t-1}^2)))\n",
    "            # Let's rewrite the update step using the formula directly for clarity and check with P1.2\n",
    "            # x_{t-1} = A' * x_t + B' * eps_hat + C' * noise\n",
    "            # From P1.2 (continuous equivalent):\n",
    "            # A' = alpha_{t-1} / alpha_t\n",
    "            # B' = sigma_{t-1} * sqrt(1 - alpha_t^2 / alpha_{t-1}^2) - alpha_{t-1} * sigma_t / alpha_t # Check this coefficient\n",
    "            # C' = eta_t\n",
    "\n",
    "            # Using the DDIM paper formula provided:\n",
    "            # x_{t-1} = alpha_{t-1} * ( (x_t - sigma_t * eps_hat) / alpha_t ) + sqrt(sigma_{t-1}^2 - eta_t^2) * eps_hat + eta_t * eps_t\n",
    "            term1 = alpha_tm1 * ((x - sigma_t * eps_hat) / alpha_t)\n",
    "\n",
    "            # Calculate sigma_{t-1}^2 - eta_t^2 carefully\n",
    "            # eta_t^2 = sigma_tm1^2 / sigma_t^2 * (1 - alpha_t^2 / alpha_tm1^2)\n",
    "            sigma_tm1_sq_minus_eta_t_sq = sigma_tm1**2 - (sigma_tm1**2 / sigma_t**2) * (1 - ratio_alpha_sq)\n",
    "            # Clip to avoid sqrt of negative\n",
    "            sigma_tm1_sq_minus_eta_t_sq_clipped = torch.clamp(sigma_tm1_sq_minus_eta_t_sq, min=0.0)\n",
    "            term2_coeff = torch.sqrt(sigma_tm1_sq_minus_eta_t_sq_clipped)\n",
    "            term2 = term2_coeff * eps_hat\n",
    "\n",
    "            # Sample random noise eps_t ~ N(0, I) only if eta_t > 0\n",
    "            term3 = 0.0\n",
    "            if tm1_ > 1e-4: # Only add noise if not the last step (t-1 is not approx 0)\n",
    "                eps_t = torch.randn_like(x, device=DEVICE)\n",
    "                term3 = eta_t * eps_t\n",
    "\n",
    "            # DDPM Update\n",
    "            x = term1 + term2 + term3\n",
    "\n",
    "        self.model.train() # Switch back to train mode\n",
    "        return x\n",
    "\n",
    "    def __getattr__(self, name):\n",
    "        if name in ['train', 'eval', 'parameters', 'state_dict', 'load_state_dict']:\n",
    "            return getattr(self.model, name)\n",
    "        return self.__getattribute__(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "737f13e3",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, n_hidden_layers, timestep_dim=1):\n",
    "        super().__init__()\n",
    "        self.timestep_dim = timestep_dim\n",
    "        prev_dim = input_dim + timestep_dim\n",
    "        net = []\n",
    "        dims = [hidden_dim] * n_hidden_layers + [input_dim]\n",
    "        for i, dim in enumerate(dims):\n",
    "            net.append(nn.Linear(prev_dim, dim))\n",
    "            if i < len(dims) - 1:\n",
    "                net.append(nn.ReLU())\n",
    "            prev_dim = dim\n",
    "        self.net = nn.Sequential(*net)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        x = torch.cat([x, t.reshape(-1, 1)], dim=1) # Ensure t has shape (batch_size, 1)\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7511e26",
   "metadata": {},
   "source": [
    "--- Training Loop Components ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "54dca569",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr(step, total_steps, warmup_steps, lr_init, use_cos_decay):\n",
    "    \"\"\"\n",
    "    Function that returns the learning rate for the specific step.\n",
    "    Handles linear warmup and optional cosine decay.\n",
    "    \"\"\"\n",
    "    if step < warmup_steps:\n",
    "        # Linear warmup\n",
    "        lr = lr_init * (step + 1) / warmup_steps\n",
    "    elif use_cos_decay:\n",
    "        # Cosine decay after warmup\n",
    "        progress = (step - warmup_steps) / max(1, total_steps - warmup_steps) # Avoid division by zero\n",
    "        lr = lr_init * 0.5 * (1.0 + math.cos(math.pi * progress))\n",
    "    else:\n",
    "        # Constant learning rate after warmup\n",
    "        lr = lr_init\n",
    "    # Ensure lr doesn't go below a minimum value if using cosine decay (e.g., decay to 0)\n",
    "    if use_cos_decay:\n",
    "         # The formula naturally decays towards 0 at total_steps\n",
    "         pass # lr can be 0 at the end\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03980c57",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer, scheduler):\n",
    "    \"\"\"\n",
    "    model: model to train, the Diffusion class in this case.\n",
    "    train_loader: dataloader for the train_data after normalization\n",
    "    optimizer: use torch.optim.Adam\n",
    "    scheduler: use optim.lr_scheduler.LambdaLR\n",
    "    Return:\n",
    "        Tensor with train loss of each batch\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    for batch in tqdm(train_loader, desc=\"Training Batch\", leave=False):\n",
    "        x = batch[0].to(DEVICE) # Assuming dataloader yields tuples/lists\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = model.loss(x)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step() # Step the scheduler each batch\n",
    "\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "    return torch.tensor(train_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa64d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def eval_loss(model, data_loader):\n",
    "    \"\"\"\n",
    "    model: model to train, the Diffusion class in this case.\n",
    "    data_loader: dataloader for the test_data after normalization\n",
    "    Return:\n",
    "        Scalar with the average test loss of each batch\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    count = 0\n",
    "    for batch in data_loader:\n",
    "        x = batch[0].to(DEVICE)\n",
    "        loss = model.loss(x)\n",
    "        total_loss += loss.item() * x.shape[0]\n",
    "        count += x.shape[0]\n",
    "    return total_loss / count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0663f80f",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def train_epochs(model, train_loader, test_loader, train_args):\n",
    "    \"\"\"\n",
    "    model: model to train, the Diffusion class in this case.\n",
    "    train_loader: dataloader for the train_data after normalization\n",
    "    test_loader: dataloader for the test_data after normalization\n",
    "    Return:\n",
    "        Two np.array for all the train losses and test losses at each step\n",
    "    \"\"\"\n",
    "    epochs, lr_init = train_args['epochs'], train_args['lr']\n",
    "    warmup_steps = train_args.get('warmup', 0)\n",
    "    use_cos_decay = train_args.get('use_cos_decay', False)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr_init)\n",
    "\n",
    "    total_steps = epochs * len(train_loader)\n",
    "\n",
    "    # Define scheduler using get_lr\n",
    "    scheduler = optim.lr_scheduler.LambdaLR(\n",
    "        optimizer,\n",
    "        lr_lambda=lambda step: get_lr(step, total_steps, warmup_steps, lr_init, use_cos_decay)\n",
    "    )\n",
    "\n",
    "    all_train_losses = []\n",
    "    all_test_losses = []\n",
    "\n",
    "    # Initial test loss\n",
    "    initial_test_loss = eval_loss(model, test_loader)\n",
    "    all_test_losses.append(initial_test_loss)\n",
    "    print(f\"Epoch 0 (Initial), Test Loss: {initial_test_loss:.4f}\")\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        epoch_train_losses = train(model, train_loader, optimizer, scheduler)\n",
    "        all_train_losses.extend(epoch_train_losses.cpu().numpy()) # Store individual batch losses\n",
    "\n",
    "        epoch_test_loss = eval_loss(model, test_loader)\n",
    "        all_test_losses.append(epoch_test_loss)\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Test Loss: {epoch_test_loss:.4f}, LR: {scheduler.get_last_lr()[0]:.6f}\")\n",
    "\n",
    "    return np.array(all_train_losses), np.array(all_test_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab81e87",
   "metadata": {},
   "source": [
    "--- Main Function and Plotting ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4040d46c",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def toy_diffusion(train_data, test_data):\n",
    "    \"\"\"\n",
    "    train_data: A (100000, 2) numpy array of 2D points\n",
    "    test_data: A (10000, 2) numpy array of 2D points\n",
    "\n",
    "    Returns\n",
    "    - a (# of training iterations,) numpy array of train losses evaluated every minibatch\n",
    "    - a (# of num_epochs + 1,) numpy array of test losses evaluated at the start of training and the end of every epoch\n",
    "    - a numpy array of size (9, 2000, 2) of samples drawn from your model.\n",
    "      Draw 2000 samples for each of 9 different number of diffusion sampling steps\n",
    "      of evenly logarithmically spaced integers 1 to 256 (corrected from 512 based on hint)\n",
    "      hint: np.power(2, np.linspace(0, 8, 9)).astype(int)\n",
    "    - train_data_mean: np.array used for normalization\n",
    "    - train_data_std: np.array used for normalization\n",
    "    \"\"\"\n",
    "    # --- Hyperparameters ---\n",
    "    batch_size = 1024\n",
    "    epochs = 100\n",
    "    lr = 1e-3\n",
    "    hidden_dim = 64\n",
    "    n_hidden_layers = 4\n",
    "    warmup_steps = 100\n",
    "    use_cos_decay = True\n",
    "    num_samples_to_gen = 2000\n",
    "    sample_steps_list = np.power(2, np.linspace(0, 8, 9)).astype(int) # 1, 2, 4, ..., 256\n",
    "\n",
    "    # --- Data Preparation ---\n",
    "    # Normalize data\n",
    "    train_data_mean = train_data.mean(axis=0)\n",
    "    train_data_std = train_data.std(axis=0)\n",
    "    train_data_normalized = (train_data - train_data_mean) / train_data_std\n",
    "    test_data_normalized = (test_data - train_data_mean) / train_data_std\n",
    "\n",
    "    # Create TensorDatasets and DataLoaders\n",
    "    train_dataset = TensorDataset(torch.from_numpy(train_data_normalized))\n",
    "    test_dataset = TensorDataset(torch.from_numpy(test_data_normalized))\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "    # --- Model Initialization ---\n",
    "    data_shape = train_data.shape[1:] # Should be (2,)\n",
    "    input_dim = np.prod(data_shape).item() # Should be 2\n",
    "\n",
    "    mlp_model = MLP(input_dim=input_dim, hidden_dim=hidden_dim, n_hidden_layers=n_hidden_layers)\n",
    "    diffusion_model = Diffusion(model=mlp_model, data_shape=data_shape)\n",
    "\n",
    "    # --- Training ---\n",
    "    train_args = {\n",
    "        'epochs': epochs,\n",
    "        'lr': lr,\n",
    "        'warmup': warmup_steps,\n",
    "        'use_cos_decay': use_cos_decay\n",
    "    }\n",
    "\n",
    "    print(\"Starting training...\")\n",
    "    all_train_losses, all_test_losses = train_epochs(\n",
    "        diffusion_model,\n",
    "        train_loader,\n",
    "        test_loader,\n",
    "        train_args\n",
    "    )\n",
    "    print(\"Training finished.\")\n",
    "\n",
    "    # --- Sampling ---\n",
    "    print(\"Generating samples...\")\n",
    "    all_samples = []\n",
    "    diffusion_model.eval() # Ensure model is in eval mode for sampling\n",
    "    for num_steps in tqdm(sample_steps_list, desc=\"Sampling Steps\"):\n",
    "        samples_normalized = diffusion_model.sample(n=num_samples_to_gen, num_steps=num_steps)\n",
    "        # Samples are on DEVICE, move to CPU and convert to numpy\n",
    "        all_samples.append(samples_normalized.cpu().numpy())\n",
    "\n",
    "    all_samples_np = np.stack(all_samples) # Shape: (9, num_samples_to_gen, 2)\n",
    "\n",
    "    # We need to return the mean and std for denormalization during plotting\n",
    "    return all_train_losses, all_test_losses, all_samples_np, train_data_mean, train_data_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84995a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def savefig(fname: str, show_figure: bool = True) -> None:\n",
    "    if not exists(dirname(fname)):\n",
    "        os.makedirs(dirname(fname))\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(fname)\n",
    "    if show_figure:\n",
    "        plt.show()\n",
    "    plt.close() # Close the figure after saving/showing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff1e66a",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def save_training_plot(\n",
    "    train_losses: np.ndarray, test_losses: np.ndarray, title: str, fname: str\n",
    ") -> None:\n",
    "    plt.figure()\n",
    "    n_epochs = len(test_losses) - 1\n",
    "    x_train = np.linspace(0, n_epochs, len(train_losses))\n",
    "    x_test = np.arange(n_epochs + 1)\n",
    "\n",
    "    plt.plot(x_train, train_losses, label=\"train loss\")\n",
    "    plt.plot(x_test, test_losses, label=\"test loss\")\n",
    "    plt.legend()\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss (MSE)\") # Usually MSE for diffusion\n",
    "    savefig(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91893cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_multi_scatter_2d(data: np.ndarray, data_mean=None, data_std=None) -> None:\n",
    "    fig, axs = plt.subplots(3, 3, figsize=(10, 10))\n",
    "    # num_steps = np.power(2, np.linspace(0, 9, 9)).astype(int) # This includes 2^9 = 512\n",
    "    num_steps = np.power(2, np.linspace(0, 8, 9)).astype(int) # Correcting based on hint, goes up to 2^8 = 256\n",
    "\n",
    "    if data_mean is not None and data_std is not None:\n",
    "        # Denormalize data for plotting\n",
    "        data = data * data_std + data_mean\n",
    "\n",
    "    for i in range(3):\n",
    "        for j in range(3):\n",
    "            idx = i * 3 + j\n",
    "            axs[i, j].scatter(data[idx, :, 0], data[idx, :, 1], s=5, alpha=0.7) # Adjust point size and alpha\n",
    "            axs[i, j].set_title(f'Steps = {num_steps[idx]}')\n",
    "            axs[i, j].set_aspect('equal', adjustable='box') # Ensure aspect ratio is equal\n",
    "            axs[i, j].set_xlim(-2.5, 2.5) # Set consistent limits if possible\n",
    "            axs[i, j].set_ylim(-1.5, 1.5)\n",
    "    savefig(\"results/p2_toy_samples.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05229186",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def toy_save_results(fn):\n",
    "    train_data = toy_2d_data(n=100000)\n",
    "    test_data = toy_2d_data(n=10000)\n",
    "    train_losses, test_losses, samples, data_mean, data_std = fn(train_data, test_data) # fn needs to return mean/std\n",
    "\n",
    "    print(f\"Final Test Loss: {test_losses[-1]:.4f}\")\n",
    "\n",
    "    save_training_plot(\n",
    "        train_losses,\n",
    "        test_losses,\n",
    "        f\"P2 Train Plot\",\n",
    "        f\"results/p2_train_plot.png\"\n",
    "    )\n",
    "\n",
    "    save_multi_scatter_2d(samples, data_mean, data_std) # Pass mean/std for denormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14e8c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    toy_save_results(toy_diffusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a3ef5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Submission Guideline for Part 2\n",
    "\n",
    "Please include your answer to all problems, including formulas, proofs, and the figures generated in each problem, excluding code. You are required to submit the (single) pdf and all (four) notebooks (one for each problem) with your code and running outputs. Do not include code in the pdf file.\n",
    "\n",
    "Specifically, for Problem 2 in this notebook, the pdf should contain:\n",
    "- The generated figures `results/p2_train_plot.png` and `results/p2_toy_samples.png`\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# -*- coding: utf-8 -*-",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "gradcv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
