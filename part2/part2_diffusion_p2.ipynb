{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a677f1d",
   "metadata": {},
   "source": [
    "# Acknowledgement\n",
    "\n",
    "Parts of this pset were inspired by\n",
    "* Berkeley CS294-158, taught by Pieter Abbeel, Wilson Yan, Kevin Frans, and Philipp Wu;\n",
    "* MIT 6.S184/6.S975, taught by Peter Holderrieth and Ezra Erives;\n",
    "* The [blog post](https://lilianweng.github.io/posts/2021-07-11-diffusion-models/) about diffusion models by Lilian Weng."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84965bb",
   "metadata": {},
   "source": [
    "# Submission Guideline for Part 2\n",
    "\n",
    "Please include your answer to all problems, including formulas, proofs, and the figures generated in each problem, excluding code. You are required to submit the (single) pdf and all (four) notebooks (one for each problem) with your code and running outputs. Do not include code in the pdf file. \n",
    "\n",
    "Specifically, for Problem 2 in this notebook, the pdf should contain:\n",
    "- The generated figures `results/p2_train_plot.png` and `results/p2_toy_samples.png`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29d1472",
   "metadata": {},
   "source": [
    "# Problem 2: Training Diffusion Models on a Toy Dataset\n",
    "In this problem, we will write the code for training and sampling from a diffusion model on a 2D toy dataset. This part requires GPUs--you can use Google Colab for GPU access. To work on this notebook in Google Colab, copy the `pset-5` directory to your Google Drive and open this notebook. Then, start working on a GPU machine with `Runtime -> Change runtime type -> T4 GPU`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d18f58",
   "metadata": {},
   "source": [
    "## Data Generation and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55c76481",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_s_curve\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from os.path import exists, dirname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02457cf8",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1d98d39",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def toy_2d_data(n=100000):\n",
    "    x, _ = make_s_curve(n, noise=0.1)\n",
    "    x = x[:, [0, 2]]\n",
    "    return x.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7fe31fce",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def visualize_toy_2d_dataset():\n",
    "    data = toy_2d_data()\n",
    "    plt.figure()\n",
    "    plt.scatter(data[:, 0], data[:, 1])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84b0e16e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualize_toy_2d_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf6ef36",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "source": [
    "## Training and Sampling of Diffusion Models\n",
    "\n",
    "For code simplicity, we will train a continuous-time variant of the diffusion prompt. In practice training objectives and code between discrete-time and continuous-time diffusion models are similar.\n",
    "\n",
    "Given a data element $x$ and neural net $f_\\theta(x, t)$, implement the following diffusion training steps:\n",
    "\n",
    "0. Construct a class `Diffusion`\n",
    "1. Sample the diffusion timestep: $t \\sim \\text{Uniform}(0, 1)$\n",
    "2. Compute the noise-strength following a cosine schedule: $\\alpha_t = \\cos\\left(\\frac{\\pi}{2}t\\right), \\sigma_t = \\sin\\left(\\frac{\\pi}{2}t\\right)$\n",
    "3. Sample noise $\\epsilon \\sim N(0,I)$ (same shape as $x$) and cmpute noised $x_t = \\alpha_t x + \\sigma_t \\epsilon$\n",
    "4. Estimate $\\hat{\\epsilon} = f_\\theta(x_t, t)$\n",
    "5. Optimize the loss $L = \\lVert \\epsilon - \\hat{\\epsilon} \\rVert_2^2$. Here, it suffices to just take the mean over all dimensions.\n",
    "\n",
    "Note that for the case of continuous-time diffusion, the forward process is $x_{0\\to1}$ and reverse process is $x_{1\\to0}$\n",
    "\n",
    "Use an MLP for $f_\\theta$ to optimize the loss. You may find the following details helpful.\n",
    "* Normalize the data using mean and std computed from the train dataset\n",
    "* Train 100 epochs, batch size 1024, Adam with LR 1e-3 (100 warmup steps, cosine decay to 0)\n",
    "* MLP with 4 hidden layers and hidden size 64\n",
    "* Condition on t by concatenating it with input x (i.e. 2D x + 1D t = 3D cat(x, t))\n",
    "* Training 100 epochs takes about 2 minutes on the Google Colab T4 GPU.\n",
    "\n",
    "\n",
    "To sample, implement the standard DDPM sampler. You may find the equation from the [DDIM paper](https://arxiv.org/pdf/2010.02502.pdf) helpful, rewritten and re-formatted here for convenience.\n",
    "$$x_{t-1} = \\alpha_{t-1}\\left(\\frac{x_t - \\sigma_t\\hat{\\epsilon}}{\\alpha_t}\\right) + \\sqrt{\\sigma_{t-1}^2 - \\eta_t^2}\\hat{\\epsilon} + \\eta_t\\epsilon_t$$\n",
    "where $\\epsilon_t \\sim N(0, I)$ is random Gaussian noise. For DDPM, let\n",
    "$$\\eta_t = \\sigma_{t-1}/\\sigma_t\\sqrt{1 - \\alpha_t^2/\\alpha_{t-1}^2}$$\n",
    "*Note*: As a sanity check, when $\\eta_t$ follows the DDPM setting as shown above, the resulted coefficient of $x_t$ and $\\hat{\\epsilon}$ should be the same as $A'$ and $B'$ you derived in Problem 1.2 (6) where $\\alpha_t$ here corresponds to $\\bar{\\alpha_t}$ in Problem 1.\n",
    "\n",
    "To run the reverse process, start from $x_1 \\sim N(0, I)$. Then perform `num_steps` DDPM updates (a hyperparameter), pseudocode below.\n",
    "```\n",
    "ts = linspace(1 - 1e-4, 1e-4, num_steps + 1)\n",
    "x = sample_normal\n",
    "for i in range(num_steps):\n",
    "    t = ts[i]\n",
    "    tm1 = ts[i + 1]\n",
    "    eps_hat = model(x, t)\n",
    "    x = DDPM_UPDATE(x, eps_hat, t, tm1)\n",
    "return x\n",
    "```\n",
    "*Note*:\n",
    "* If you encounter NaNs, you may need to clip $\\sigma_{t-1}^2 - \\eta_t^2$ to 0 if it goes negative, as machine precision issues can make it a very small negative number (e.g. -1e-12) if its too close to 0\n",
    "* For debugging, you can start with trying small number of epochs. To debug your training code, you can check whether the training and testing losses decrease properly. To debug your sampling code, you can check whether the generated distribution is close to the S-shape target distribution with large `num_steps`.\n",
    "* To check your answer, the final text loss is roughly around 0.4."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43881b0d",
   "metadata": {},
   "source": [
    "--- Diffusion Model Components ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6585078a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Diffusion:\n",
    "    def __init__(self, model, data_shape):\n",
    "        \"\"\"\n",
    "        model: neural network to estimate eps_hat (MLP in this problem)\n",
    "        data_shape: size of the input data, (2,) in this case\n",
    "        \"\"\"\n",
    "        self.model = model.to(DEVICE)\n",
    "        self.data_shape = data_shape\n",
    "\n",
    "    def loss(self, x):\n",
    "        \"\"\"\n",
    "        x: the input data (without adding noise) from the dataloader\n",
    "        Return:\n",
    "            The loss (as a scalar averaged over all data in the batch)\n",
    "        \"\"\"\n",
    "        t = torch.rand(x.shape[0], device=DEVICE)\n",
    "\n",
    "        alpha_t, sigma_t = torch.cos((math.pi / 2) ** t), torch.sin((math.pi / 2) ** t)\n",
    "        epsilon = torch.randn_like(x, device=DEVICE)\n",
    "        x_t = alpha_t.unsqueeze(-1) * x + sigma_t.unsqueeze(-1) * epsilon\n",
    "\n",
    "        eps_hat = self.model(x_t, t)\n",
    "\n",
    "        loss = torch.mean((epsilon - eps_hat)**2)\n",
    "        return loss\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sample(self, n, num_steps):\n",
    "        \"\"\"\n",
    "        n: number of samples to generate\n",
    "        num_steps: number of steps in the diffusion sampling\n",
    "        Return:\n",
    "            The generated sample. Tensor with shape (n, *self.data_shape)\n",
    "        \"\"\"\n",
    "        ts = torch.linspace(1.0 - 1e-4, 1e-4, num_steps + 1, device=DEVICE)\n",
    "        x = torch.randn(n, *self.data_shape, device=DEVICE)\n",
    "\n",
    "        for i in range(num_steps):\n",
    "            t_current = ts[i]\n",
    "            alpha_t, sigma_t = torch.cos((math.pi / 2) * t_current), torch.sin((math.pi / 2) * t_current)\n",
    "\n",
    "            t_next = ts[i + 1]\n",
    "            alpha_next, sigma_next = torch.cos((math.pi / 2) * t_next), torch.sin((math.pi / 2) * t_next)\n",
    "\n",
    "            t_expanded = t_current.expand(n)\n",
    "            eps_pred = self.model(x, t_expanded)\n",
    "            \n",
    "            sigma_t = torch.clamp(sigma_t, min=1e-12)\n",
    "            sigma_next = torch.clamp(sigma_next, min=1e-12)\n",
    "            \n",
    "            noise_coef = (sigma_next / sigma_t) * torch.sqrt(torch.clamp(1 - (alpha_t**2) / (alpha_next**2), min=0.0))\n",
    "            \n",
    "            x = (alpha_next * ((x - sigma_t * eps_pred) / alpha_t)) + \\\n",
    "                torch.sqrt(torch.clamp(sigma_next**2 - noise_coef**2, min=0.0)) * eps_pred + \\\n",
    "                noise_coef * torch.randn_like(x)\n",
    "            \n",
    "        return x\n",
    "\n",
    "    def __getattr__(self, name):\n",
    "        if name in ['train', 'eval', 'parameters', 'state_dict', 'load_state_dict']:\n",
    "            return getattr(self.model, name)\n",
    "        return self.__getattribute__(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "737f13e3",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, n_hidden_layers, timestep_dim=1):\n",
    "        super().__init__()\n",
    "        self.timestep_dim = timestep_dim\n",
    "        prev_dim = input_dim + timestep_dim\n",
    "        net = []\n",
    "        dims = [hidden_dim] * n_hidden_layers + [input_dim]\n",
    "        for i, dim in enumerate(dims):\n",
    "            net.append(nn.Linear(prev_dim, dim))\n",
    "            if i < len(dims) - 1:\n",
    "                net.append(nn.ReLU())\n",
    "            prev_dim = dim\n",
    "        self.net = nn.Sequential(*net)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        x = torch.cat([x, t.reshape(-1, 1)], dim=1) # Ensure t has shape (batch_size, 1)\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7511e26",
   "metadata": {},
   "source": [
    "--- Training Loop Components ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "54dca569",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr(step, total_steps, warmup_steps, lr_init, use_cos_decay):\n",
    "    \"\"\"\n",
    "    Function that returns the learning rate for the specific step.\n",
    "    Handles linear warmup and optional cosine decay.\n",
    "    \"\"\"\n",
    "    if step < warmup_steps:\n",
    "        lr = lr_init * (step + 1) / warmup_steps\n",
    "    elif use_cos_decay:\n",
    "        progress = (step - warmup_steps) / max(1, total_steps - warmup_steps)\n",
    "        lr = lr_init * 0.5 * (1.0 + math.cos(math.pi * progress))\n",
    "    else:\n",
    "        return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "03980c57",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer, scheduler):\n",
    "    \"\"\"\n",
    "    model: model to train, the Diffusion class in this case.\n",
    "    train_loader: dataloader for the train_data after normalization\n",
    "    optimizer: use torch.optim.Adam\n",
    "    scheduler: use optim.lr_scheduler.LambdaLR\n",
    "    Return:\n",
    "        Tensor with train loss of each batch\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    for batch in tqdm(train_loader, desc=\"Training Batch\", leave=False):\n",
    "        x = batch[0].to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = model.loss(x)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "    return torch.tensor(train_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7fa64d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def eval_loss(model, data_loader):\n",
    "    \"\"\"\n",
    "    model: model to train, the Diffusion class in this case.\n",
    "    data_loader: dataloader for the test_data after normalization\n",
    "    Return:\n",
    "        Scalar with the average test loss of each batch\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    count = 0\n",
    "    for batch in data_loader:\n",
    "        x = batch[0].to(DEVICE)\n",
    "        loss = model.loss(x)\n",
    "        total_loss += loss.item() * x.shape[0]\n",
    "        count += x.shape[0]\n",
    "    return total_loss / count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0663f80f",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def train_epochs(model, train_loader, test_loader, train_args):\n",
    "    \"\"\"\n",
    "    model: model to train, the Diffusion class in this case.\n",
    "    train_loader: dataloader for the train_data after normalization\n",
    "    test_loader: dataloader for the test_data after normalization\n",
    "    Return:\n",
    "        Two np.array for all the train losses and test losses at each step\n",
    "    \"\"\"\n",
    "    epochs, lr_init = train_args['epochs'], train_args['lr']\n",
    "    warmup_steps = train_args.get('warmup', 0)\n",
    "    use_cos_decay = train_args.get('use_cos_decay', False)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr_init)\n",
    "\n",
    "    total_steps = epochs * len(train_loader)\n",
    "\n",
    "    scheduler = optim.lr_scheduler.LambdaLR(\n",
    "        optimizer,\n",
    "        lr_lambda=lambda step: get_lr(step, total_steps, warmup_steps, lr_init, use_cos_decay)\n",
    "    )\n",
    "\n",
    "    all_train_losses = []\n",
    "    all_test_losses = []\n",
    "\n",
    "    initial_test_loss = eval_loss(model, test_loader)\n",
    "    all_test_losses.append(initial_test_loss)\n",
    "    print(f\"Epoch 0 (Initial), Test Loss: {initial_test_loss:.4f}\")\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        epoch_train_losses = train(model, train_loader, optimizer, scheduler)\n",
    "        all_train_losses.extend(epoch_train_losses.cpu().numpy())\n",
    "\n",
    "        epoch_test_loss = eval_loss(model, test_loader)\n",
    "        all_test_losses.append(epoch_test_loss)\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Test Loss: {epoch_test_loss:.4f}, LR: {scheduler.get_last_lr()[0]:.6f}\")\n",
    "\n",
    "    return np.array(all_train_losses), np.array(all_test_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab81e87",
   "metadata": {},
   "source": [
    "--- Main Function and Plotting ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4040d46c",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def toy_diffusion(train_data, test_data):\n",
    "    \"\"\"\n",
    "    train_data: A (100000, 2) numpy array of 2D points\n",
    "    test_data: A (10000, 2) numpy array of 2D points\n",
    "\n",
    "    Returns\n",
    "    - a (# of training iterations,) numpy array of train losses evaluated every minibatch\n",
    "    - a (# of num_epochs + 1,) numpy array of test losses evaluated at the start of training and the end of every epoch\n",
    "    - a numpy array of size (9, 2000, 2) of samples drawn from your model.\n",
    "      Draw 2000 samples for each of 9 different number of diffusion sampling steps\n",
    "      of evenly logarithmically spaced integers 1 to 256 (corrected from 512 based on hint)\n",
    "      hint: np.power(2, np.linspace(0, 8, 9)).astype(int)\n",
    "    - train_data_mean: np.array used for normalization\n",
    "    - train_data_std: np.array used for normalization\n",
    "    \"\"\"\n",
    "    batch_size = 1024\n",
    "    epochs = 100\n",
    "    lr = 1e-3\n",
    "    hidden_dim = 64\n",
    "    n_hidden_layers = 4\n",
    "    warmup_steps = 100\n",
    "    use_cos_decay = True\n",
    "    num_samples_to_gen = 2000\n",
    "    sample_steps_list = np.power(2, np.linspace(0, 8, 9)).astype(int)\n",
    "\n",
    "    train_data_mean = train_data.mean(axis=0)\n",
    "    train_data_std = train_data.std(axis=0)\n",
    "    train_data_normalized = (train_data - train_data_mean) / train_data_std\n",
    "    test_data_normalized = (test_data - train_data_mean) / train_data_std\n",
    "\n",
    "    train_dataset = TensorDataset(torch.from_numpy(train_data_normalized))\n",
    "    test_dataset = TensorDataset(torch.from_numpy(test_data_normalized))\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "    data_shape = train_data.shape[1:]\n",
    "    input_dim = np.prod(data_shape).item()\n",
    "\n",
    "    mlp_model = MLP(input_dim=input_dim, hidden_dim=hidden_dim, n_hidden_layers=n_hidden_layers)\n",
    "    diffusion_model = Diffusion(model=mlp_model, data_shape=data_shape)\n",
    "\n",
    "    train_args = {\n",
    "        'epochs': epochs,\n",
    "        'lr': lr,\n",
    "        'warmup': warmup_steps,\n",
    "        'use_cos_decay': use_cos_decay\n",
    "    }\n",
    "\n",
    "    print(\"Starting training...\")\n",
    "    all_train_losses, all_test_losses = train_epochs(\n",
    "        diffusion_model,\n",
    "        train_loader,\n",
    "        test_loader,\n",
    "        train_args\n",
    "    )\n",
    "    print(\"Training finished.\")\n",
    "\n",
    "    print(\"Generating samples...\")\n",
    "    all_samples = []\n",
    "    diffusion_model.eval()\n",
    "    for num_steps in tqdm(sample_steps_list, desc=\"Sampling Steps\"):\n",
    "        samples_normalized = diffusion_model.sample(n=num_samples_to_gen, num_steps=num_steps)\n",
    "        all_samples.append(samples_normalized.cpu().numpy())\n",
    "\n",
    "    all_samples_np = np.stack(all_samples)\n",
    "\n",
    "    return all_train_losses, all_test_losses, all_samples_np, train_data_mean, train_data_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "84995a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def savefig(fname: str, show_figure: bool = True) -> None:\n",
    "    if not exists(dirname(fname)):\n",
    "        os.makedirs(dirname(fname))\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(fname)\n",
    "    if show_figure:\n",
    "        plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cff1e66a",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def save_training_plot(\n",
    "    train_losses: np.ndarray, test_losses: np.ndarray, title: str, fname: str\n",
    ") -> None:\n",
    "    plt.figure()\n",
    "    n_epochs = len(test_losses) - 1\n",
    "    x_train = np.linspace(0, n_epochs, len(train_losses))\n",
    "    x_test = np.arange(n_epochs + 1)\n",
    "\n",
    "    plt.plot(x_train, train_losses, label=\"train loss\")\n",
    "    plt.plot(x_test, test_losses, label=\"test loss\")\n",
    "    plt.legend()\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss (MSE)\")\n",
    "    savefig(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "91893cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_multi_scatter_2d(data: np.ndarray, data_mean=None, data_std=None) -> None:\n",
    "    fig, axs = plt.subplots(3, 3, figsize=(10, 10))\n",
    "    num_steps = np.power(2, np.linspace(0, 8, 9)).astype(int)\n",
    "\n",
    "    if data_mean is not None and data_std is not None:\n",
    "        data = data * data_std + data_mean\n",
    "\n",
    "    for i in range(3):\n",
    "        for j in range(3):\n",
    "            idx = i * 3 + j\n",
    "            axs[i, j].scatter(data[idx, :, 0], data[idx, :, 1], s=5, alpha=0.7)\n",
    "            axs[i, j].set_title(f'Steps = {num_steps[idx]}')\n",
    "            axs[i, j].set_aspect('equal', adjustable='box')\n",
    "            axs[i, j].set_xlim(-2.5, 2.5)\n",
    "            axs[i, j].set_ylim(-1.5, 1.5)\n",
    "    savefig(\"results/p2_toy_samples.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "05229186",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def toy_save_results(fn):\n",
    "    train_data = toy_2d_data(n=100000)\n",
    "    test_data = toy_2d_data(n=10000)\n",
    "    train_losses, test_losses, samples, data_mean, data_std = fn(train_data, test_data) # fn needs to return mean/std\n",
    "\n",
    "    print(f\"Final Test Loss: {test_losses[-1]:.4f}\")\n",
    "\n",
    "    save_training_plot(\n",
    "        train_losses,\n",
    "        test_losses,\n",
    "        f\"P2 Train Plot\",\n",
    "        f\"results/p2_train_plot.png\"\n",
    "    )\n",
    "\n",
    "    save_multi_scatter_2d(samples, data_mean, data_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b14e8c88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Epoch 0 (Initial), Test Loss: 0.9839\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Test Loss: 1.0015, LR: 0.000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100, Test Loss: 1.0016, LR: 0.000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/100, Test Loss: 1.0203, LR: 0.000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/100, Test Loss: 0.9921, LR: 0.000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/100, Test Loss: 0.9867, LR: 0.000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/100, Test Loss: 0.9792, LR: 0.000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/100, Test Loss: 1.0035, LR: 0.000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/100, Test Loss: 0.9934, LR: 0.000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m----> 2\u001b[0m     \u001b[43mtoy_save_results\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoy_diffusion\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[20], line 4\u001b[0m, in \u001b[0;36mtoy_save_results\u001b[0;34m(fn)\u001b[0m\n\u001b[1;32m      2\u001b[0m train_data \u001b[38;5;241m=\u001b[39m toy_2d_data(n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100000\u001b[39m)\n\u001b[1;32m      3\u001b[0m test_data \u001b[38;5;241m=\u001b[39m toy_2d_data(n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10000\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m train_losses, test_losses, samples, data_mean, data_std \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_data\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# fn needs to return mean/std\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinal Test Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_losses[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m save_training_plot(\n\u001b[1;32m      9\u001b[0m     train_losses,\n\u001b[1;32m     10\u001b[0m     test_losses,\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mP2 Train Plot\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresults/p2_train_plot.png\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     13\u001b[0m )\n",
      "Cell \u001b[0;32mIn[15], line 57\u001b[0m, in \u001b[0;36mtoy_diffusion\u001b[0;34m(train_data, test_data)\u001b[0m\n\u001b[1;32m     49\u001b[0m train_args \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m'\u001b[39m: epochs,\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m: lr,\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwarmup\u001b[39m\u001b[38;5;124m'\u001b[39m: warmup_steps,\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124muse_cos_decay\u001b[39m\u001b[38;5;124m'\u001b[39m: use_cos_decay\n\u001b[1;32m     54\u001b[0m }\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting training...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 57\u001b[0m all_train_losses, all_test_losses \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epochs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdiffusion_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_args\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining finished.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# --- Sampling ---\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[14], line 32\u001b[0m, in \u001b[0;36mtrain_epochs\u001b[0;34m(model, train_loader, test_loader, train_args)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch 0 (Initial), Test Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minitial_test_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m---> 32\u001b[0m     epoch_train_losses \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m     all_train_losses\u001b[38;5;241m.\u001b[39mextend(epoch_train_losses\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()) \u001b[38;5;66;03m# Store individual batch losses\u001b[39;00m\n\u001b[1;32m     35\u001b[0m     epoch_test_loss \u001b[38;5;241m=\u001b[39m eval_loss(model, test_loader)\n",
      "Cell \u001b[0;32mIn[12], line 12\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, optimizer, scheduler)\u001b[0m\n\u001b[1;32m     10\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     11\u001b[0m train_losses \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m tqdm(train_loader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining Batch\u001b[39m\u001b[38;5;124m\"\u001b[39m, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m     13\u001b[0m     x \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mto(DEVICE) \u001b[38;5;66;03m# Assuming dataloader yields tuples/lists\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:708\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 708\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    709\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    712\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    713\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    714\u001b[0m ):\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:1446\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1443\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1444\u001b[0m     \u001b[38;5;66;03m# no valid `self._rcvd_idx` is found (i.e., didn't break)\u001b[39;00m\n\u001b[1;32m   1445\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_persistent_workers:\n\u001b[0;32m-> 1446\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_shutdown_workers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1447\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n\u001b[1;32m   1449\u001b[0m \u001b[38;5;66;03m# Now `self._rcvd_idx` is the batch index we want to fetch\u001b[39;00m\n\u001b[1;32m   1450\u001b[0m \n\u001b[1;32m   1451\u001b[0m \u001b[38;5;66;03m# Check if the next sample has already been generated\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:1582\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._shutdown_workers\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1577\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mark_worker_as_unavailable(worker_id, shutdown\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1578\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_workers:\n\u001b[1;32m   1579\u001b[0m     \u001b[38;5;66;03m# We should be able to join here, but in case anything went\u001b[39;00m\n\u001b[1;32m   1580\u001b[0m     \u001b[38;5;66;03m# wrong, we set a timeout and if the workers fail to join,\u001b[39;00m\n\u001b[1;32m   1581\u001b[0m     \u001b[38;5;66;03m# they are killed in the `finally` block.\u001b[39;00m\n\u001b[0;32m-> 1582\u001b[0m     \u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMP_STATUS_CHECK_INTERVAL\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1583\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m q \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_queues:\n\u001b[1;32m   1584\u001b[0m     q\u001b[38;5;241m.\u001b[39mcancel_join_thread()\n",
      "File \u001b[0;32m/usr/lib/python3.10/multiprocessing/process.py:149\u001b[0m, in \u001b[0;36mBaseProcess.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parent_pid \u001b[38;5;241m==\u001b[39m os\u001b[38;5;241m.\u001b[39mgetpid(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcan only join a child process\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcan only join a started process\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 149\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_popen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m res \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    151\u001b[0m     _children\u001b[38;5;241m.\u001b[39mdiscard(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[0;32m/usr/lib/python3.10/multiprocessing/popen_fork.py:40\u001b[0m, in \u001b[0;36mPopen.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmultiprocessing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconnection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m wait\n\u001b[0;32m---> 40\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msentinel\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     41\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# This shouldn't block if wait() returned successfully.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/multiprocessing/connection.py:931\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    928\u001b[0m     deadline \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic() \u001b[38;5;241m+\u001b[39m timeout\n\u001b[1;32m    930\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 931\u001b[0m     ready \u001b[38;5;241m=\u001b[39m \u001b[43mselector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    932\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ready:\n\u001b[1;32m    933\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [key\u001b[38;5;241m.\u001b[39mfileobj \u001b[38;5;28;01mfor\u001b[39;00m (key, events) \u001b[38;5;129;01min\u001b[39;00m ready]\n",
      "File \u001b[0;32m/usr/lib/python3.10/selectors.py:416\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    414\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 416\u001b[0m     fd_event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    toy_save_results(toy_diffusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a3ef5b",
   "metadata": {},
   "source": [
    "# Submission Guideline for Part 2\n",
    "\n",
    "Please include your answer to all problems, including formulas, proofs, and the figures generated in each problem, excluding code. You are required to submit the (single) pdf and all (four) notebooks (one for each problem) with your code and running outputs. Do not include code in the pdf file.\n",
    "\n",
    "Specifically, for Problem 2 in this notebook, the pdf should contain:\n",
    "- The generated figures `results/p2_train_plot.png` and `results/p2_toy_samples.png`"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# -*- coding: utf-8 -*-",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
