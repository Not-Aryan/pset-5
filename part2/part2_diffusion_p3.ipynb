{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be3145fe",
   "metadata": {
    "id": "3YsSv7ZWExb6"
   },
   "source": [
    "# Acknowledgement\n",
    "\n",
    "Parts of this pset were inspired by\n",
    "* Berkeley CS294-158, taught by Pieter Abbeel, Wilson Yan, Kevin Frans, and Philipp Wu;\n",
    "* MIT 6.S184/6.S975, taught by Peter Holderrieth and Ezra Erives;\n",
    "* The [blog post](https://lilianweng.github.io/posts/2021-07-11-diffusion-models/) about diffusion models by Lilian Weng.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc53ca2b",
   "metadata": {
    "id": "YMetH7q3Exb7"
   },
   "source": [
    "# Submission Guideline for Part 2\n",
    "\n",
    "Please include your answer to all problems, including formulas, proofs, and the figures generated in each problem, excluding code. You are required to submit the (single) pdf and all (four) notebooks (one for each problem) with your code and running outputs. Do not include code in the pdf file.\n",
    "\n",
    "Specifically, for Problem 3 in this notebook, the pdf should contain:\n",
    "- The generated figures `results/mnist_train_plot.png` and `results/image_w{w}.png` (w=0.0, 0.5, 1.0, 2.0, 4.0)\n",
    "- Answer to the short answer question about the U-Net architecture\n",
    "- Answer to the short answer question about different CFG weight $w$ in problem 3.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6914933",
   "metadata": {
    "id": "pjuQYgfAnQei"
   },
   "source": [
    "# Problem 3: MNIST and Conditional Generation\n",
    "In this problem, we will write the code for conditional generation on the MNIST dataset. This part requires GPUs--you can use Google Colab for GPU access. To work on this notebook in Google Colab, copy the `pset-5` directory to your Google Drive and open this notebook. Then, start working on a GPU machine with `Runtime -> Change runtime type -> T4 GPU`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39714ee2",
   "metadata": {
    "id": "vTf_uTXrLLwP"
   },
   "source": [
    "## MNIST Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187673b8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "99dK1DZtngKq",
    "outputId": "45bdf1d7-37a8-4e9d-ef37-c2b002e8b591"
   },
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "tf = transforms.Compose([transforms.ToTensor()])\n",
    "train_dataset = MNIST(\"./data\", train=True, download=True, transform=tf)\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_dataset = MNIST(\"./data\", train=False, download=True, transform=tf)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9b1b7f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 251
    },
    "id": "a3yr1AOrOLCK",
    "outputId": "f921e106-7789-4b25-d935-301915c4c28e"
   },
   "outputs": [],
   "source": [
    "# visualize data by label\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "images_by_label = {i: [] for i in range(10)}\n",
    "\n",
    "for images, labels in train_loader:\n",
    "    for img, label in zip(images, labels):\n",
    "        if len(images_by_label[label.item()]) < 2:\n",
    "            images_by_label[label.item()].append(img.squeeze(0))\n",
    "        if all(len(images) == 2 for images in images_by_label.values()):\n",
    "            break\n",
    "    if all(len(images) == 2 for images in images_by_label.values()):\n",
    "        break\n",
    "\n",
    "# Plot the images\n",
    "fig, axes = plt.subplots(2, 10, figsize=(12, 3))\n",
    "fig.suptitle(\"MNIST Dataset\", fontsize=13, y=1.05)\n",
    "\n",
    "for label, imgs in images_by_label.items():\n",
    "    for i, img in enumerate(imgs):\n",
    "        ax = axes[i, label]\n",
    "        ax.imshow(img.numpy(), cmap=\"gray\")\n",
    "        ax.axis(\"off\")\n",
    "        if i == 0:\n",
    "            ax.set_title(f\"Label {label}\", fontsize=10)\n",
    "\n",
    "plt.tight_layout(pad=1.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639b8051",
   "metadata": {
    "id": "820hB9zhPner"
   },
   "source": [
    "## 3.1 U-Net: Architecture for Image Data\n",
    "In the toy dataset, we choose MLP as the architecture of the denoising diffusion models, and use concatenation as the way to incorporate the time embedding. Although this works fine for simple synthetic distributions, it no longer suffices for complex high-dimensional distributions like images. In this problem, we will introduce the U-Net architecture specifically designed for images.\n",
    "\n",
    "Specifically, we apply [classifier-free guidance](https://arxiv.org/pdf/2207.12598) (CFG) for conditional generation of MNIST digits, conditioned on the digit label. CFG is a widely used method during diffusion model sampling to push samples towards more accurately aligning with the conditioning information (e.g. class, text caption).\n",
    "\n",
    "When applying CFG, the label embedding, together with the time embedding, is added to each hidden layer of the U-Net. A diagram of the U-Net we'll be using is shown below (we change BatchNorm to GroupNorm for better performance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3074039b",
   "metadata": {
    "id": "-n6lTFLGFEoY"
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import torch\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from torchvision.utils import save_image, make_grid\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from matplotlib.pyplot import savefig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2683f960",
   "metadata": {
    "id": "R91fDLcLFEhh"
   },
   "outputs": [],
   "source": [
    "# --- Schedules ---\n",
    "def alpha_sigma_linear_schedule(t):\n",
    "    \"\"\"Linear schedule for alpha_t and sigma_t as per P3.2.\n",
    "    alpha_t = 1 - t\n",
    "    sigma_t = sqrt(1 - alpha_t^2)\n",
    "    \"\"\"\n",
    "    alpha_t = 1.0 - t\n",
    "    # Ensure argument to sqrt is non-negative (can happen for t approx 0 or 1)\n",
    "    sigma_t = torch.sqrt(torch.clamp(1.0 - alpha_t**2, min=1e-8))\n",
    "    return alpha_t, sigma_t\n",
    "\n",
    "# --- U-Net Components ---\n",
    "\n",
    "class FourierEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Based on https://github.com/lucidrains/denoising-diffusion-pytorch/blob/main/denoising_diffusion_pytorch/karras_unet.py#L183\n",
    "    \"\"\"\n",
    "    def __init__(self, dim: int):\n",
    "        super().__init__()\n",
    "        assert dim % 2 == 0\n",
    "        self.half_dim = dim // 2\n",
    "        self.weights = nn.Parameter(torch.randn(1, self.half_dim))\n",
    "\n",
    "    def forward(self, t: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        - t: (bs, 1, 1, 1)\n",
    "        Returns:\n",
    "        - embeddings: (bs, dim)\n",
    "        \"\"\"\n",
    "        t = t.view(-1, 1) # (bs, 1)\n",
    "        freqs = t * self.weights * 2 * math.pi # (bs, half_dim)\n",
    "        sin_embed = torch.sin(freqs) # (bs, half_dim)\n",
    "        cos_embed = torch.cos(freqs) # (bs, half_dim)\n",
    "        return torch.cat([sin_embed, cos_embed], dim=-1) * math.sqrt(2) # (bs, dim)\n",
    "\n",
    "class ResidualLayer(nn.Module):\n",
    "    def __init__(self, channels: int, time_embed_dim: int, y_embed_dim: int):\n",
    "        super().__init__()\n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.Conv2d(channels, channels, kernel_size=3, padding=1),\n",
    "            nn.GroupNorm(num_groups=8, num_channels=channels),\n",
    "            nn.SiLU(),\n",
    "        )\n",
    "        self.block2 = nn.Sequential(\n",
    "            nn.Conv2d(channels, channels, kernel_size=3, padding=1),\n",
    "            nn.GroupNorm(num_groups=8, num_channels=channels),\n",
    "            nn.SiLU(),\n",
    "        )\n",
    "        # Converts (bs, time_embed_dim) -> (bs, channels)\n",
    "        self.time_adapter = nn.Sequential(\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(time_embed_dim, channels)\n",
    "        )\n",
    "        # Converts (bs, y_embed_dim) -> (bs, channels)\n",
    "        self.y_adapter = nn.Sequential(\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(y_embed_dim, channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor, t_embed: torch.Tensor, y_embed: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        - x: (bs, c, h, w)\n",
    "        - t_embed: (bs, t_embed_dim)\n",
    "        - y_embed: (bs, y_embed_dim)\n",
    "        \"\"\"\n",
    "        res = x.clone() # (bs, c, h, w)\n",
    "\n",
    "        # Initial conv block\n",
    "        x = self.block1(x) # (bs, c, h, w)\n",
    "\n",
    "        # Add time embedding\n",
    "        t_embed = self.time_adapter(t_embed).unsqueeze(-1).unsqueeze(-1) # (bs, c, 1, 1)\n",
    "        x = x + t_embed\n",
    "\n",
    "        # Add y embedding (conditional embedding)\n",
    "        y_embed = self.y_adapter(y_embed).unsqueeze(-1).unsqueeze(-1) # (bs, c, 1, 1)\n",
    "        x = x + y_embed\n",
    "\n",
    "        # Second conv block\n",
    "        x = self.block2(x) # (bs, c, h, w)\n",
    "\n",
    "        # Add back residual\n",
    "        x = x + res # (bs, c, h, w)\n",
    "\n",
    "        return x\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, channels_in: int, channels_out: int, num_residual_layers: int, t_embed_dim: int, y_embed_dim: int):\n",
    "        super().__init__()\n",
    "        self.res_blocks = nn.ModuleList([\n",
    "            ResidualLayer(channels_in, t_embed_dim, y_embed_dim) for _ in range(num_residual_layers)\n",
    "        ])\n",
    "        self.downsample = nn.Conv2d(channels_in, channels_out, kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, t_embed: torch.Tensor, y_embed: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        - x: (bs, c_in, h, w)\n",
    "        - t_embed: (bs, t_embed_dim)\n",
    "        - y_embed: (bs, y_embed_dim)\n",
    "        \"\"\"\n",
    "        # Pass through residual blocks: (bs, c_in, h, w) -> (bs, c_in, h, w)\n",
    "        for block in self.res_blocks:\n",
    "            x = block(x, t_embed, y_embed)\n",
    "\n",
    "        # Downsample: (bs, c_in, h, w) -> (bs, c_out, h // 2, w // 2)\n",
    "        x = self.downsample(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class Midcoder(nn.Module):\n",
    "    def __init__(self, channels: int, num_residual_layers: int, t_embed_dim: int, y_embed_dim: int):\n",
    "        super().__init__()\n",
    "        self.res_blocks = nn.ModuleList([\n",
    "            ResidualLayer(channels, t_embed_dim, y_embed_dim) for _ in range(num_residual_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x: torch.Tensor, t_embed: torch.Tensor, y_embed: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        - x: (bs, c, h, w)\n",
    "        - t_embed: (bs, t_embed_dim)\n",
    "        - y_embed: (bs, y_embed_dim)\n",
    "        \"\"\"\n",
    "        # Pass through residual blocks: (bs, c, h, w) -> (bs, c, h, w)\n",
    "        for block in self.res_blocks:\n",
    "            x = block(x, t_embed, y_embed)\n",
    "\n",
    "        return x\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, channels_in: int, channels_out: int, num_residual_layers: int, t_embed_dim: int, y_embed_dim: int):\n",
    "        super().__init__()\n",
    "        self.upsample = nn.Sequential(nn.Upsample(scale_factor=2, mode='bilinear'), nn.Conv2d(channels_in, channels_out, kernel_size=3, padding=1))\n",
    "        self.res_blocks = nn.ModuleList([\n",
    "            ResidualLayer(channels_out, t_embed_dim, y_embed_dim) for _ in range(num_residual_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x: torch.Tensor, t_embed: torch.Tensor, y_embed: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        - x: (bs, c, h, w)\n",
    "        - t_embed: (bs, t_embed_dim)\n",
    "        - y_embed: (bs, y_embed_dim)\n",
    "        \"\"\"\n",
    "        # Upsample: (bs, c_in, h, w) -> (bs, c_out, 2 * h, 2 * w)\n",
    "        x = self.upsample(x)\n",
    "\n",
    "        # Pass through residual blocks: (bs, c_out, h, w) -> (bs, c_out, 2 * h, 2 * w)\n",
    "        for block in self.res_blocks:\n",
    "            x = block(x, t_embed, y_embed)\n",
    "\n",
    "        return x\n",
    "\n",
    "class MNISTUNet(nn.Module):\n",
    "    def __init__(self, channels: List[int], num_residual_layers: int, t_embed_dim: int, y_embed_dim: int):\n",
    "        super().__init__()\n",
    "        # Initial convolution: (bs, 1, 32, 32) -> (bs, c_0, 32, 32)\n",
    "        self.init_conv = nn.Sequential(\n",
    "            nn.Conv2d(1, channels[0], kernel_size=3, padding=1),\n",
    "            # nn.BatchNorm2d(channels[0]),\n",
    "            nn.GroupNorm(num_groups=8, num_channels=channels[0]),\n",
    "            nn.SiLU()\n",
    "        )\n",
    "\n",
    "        # Initialize time embedder\n",
    "        self.time_embedder = FourierEncoder(t_embed_dim)\n",
    "\n",
    "        # Initialize y embedder\n",
    "        self.y_embedder = nn.Embedding(num_embeddings = 11, embedding_dim = y_embed_dim)\n",
    "\n",
    "        # Encoders, Midcoders, and Decoders\n",
    "        encoders = []\n",
    "        decoders = []\n",
    "        for (curr_c, next_c) in zip(channels[:-1], channels[1:]):\n",
    "            encoders.append(Encoder(curr_c, next_c, num_residual_layers, t_embed_dim, y_embed_dim))\n",
    "            decoders.append(Decoder(next_c, curr_c, num_residual_layers, t_embed_dim, y_embed_dim))\n",
    "        self.encoders = nn.ModuleList(encoders)\n",
    "        self.decoders = nn.ModuleList(reversed(decoders))\n",
    "\n",
    "        self.midcoder1 = Midcoder(channels[-1], num_residual_layers, t_embed_dim, y_embed_dim)\n",
    "        self.midcoder2 = Midcoder(channels[-1], num_residual_layers, t_embed_dim, y_embed_dim)\n",
    "\n",
    "        # Final convolution\n",
    "        self.final_conv = nn.Sequential(\n",
    "            nn.GroupNorm(num_groups=8, num_channels=channels[0]),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(channels[0], 1, kernel_size=3, padding=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor, t: torch.Tensor, y: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        - x: (bs, 1, 32, 32)\n",
    "        - t: (bs, 1, 1, 1)\n",
    "        - y: (bs,)\n",
    "        Returns:\n",
    "        - u_t^theta(x|y): (bs, 1, 32, 32)\n",
    "        \"\"\"\n",
    "        # Embed t and y\n",
    "        t_embed = self.time_embedder(t) # (bs, time_embed_dim)\n",
    "        y_embed = self.y_embedder(y) # (bs, y_embed_dim)\n",
    "\n",
    "        # Initial convolution\n",
    "        x = self.init_conv(x) # (bs, c_0, 32, 32)\n",
    "\n",
    "        residuals = []\n",
    "\n",
    "        # Encoders\n",
    "        for encoder in self.encoders:\n",
    "            x = encoder(x, t_embed, y_embed) # (bs, c_i, h, w) -> (bs, c_{i+1}, h // 2, w //2)\n",
    "            residuals.append(x.clone())\n",
    "\n",
    "        # Midcoder\n",
    "        x = self.midcoder1(x, t_embed, y_embed)\n",
    "        x = self.midcoder2(x, t_embed, y_embed)\n",
    "\n",
    "        # Decoders\n",
    "        for decoder in self.decoders:\n",
    "            res = residuals.pop() # (bs, c_i, h, w)\n",
    "            x = x + res\n",
    "            x = decoder(x, t_embed, y_embed) # (bs, c_i, h, w) -> (bs, c_{i-1}, 2 * h, 2 * w)\n",
    "\n",
    "        # Final convolution\n",
    "        x = self.final_conv(x) # (bs, 1, 32, 32)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362d4ae6",
   "metadata": {
    "id": "Q8R9X1FHbuIy"
   },
   "source": [
    "**Please explain each components of the architecture above** (each one of `FourierEncoder`, `ResidualLayer`, `Encoder`, `Decoder`, or `Midcoder`) in your own words, (1) their role in the U-Net, (2) their inputs and outputs, and (3) a brief description of how the inputs turn into outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1ad528",
   "metadata": {
    "id": "short_answer_unet"
   },
   "source": [
    "**Answer:**\n",
    "\n",
    "1.  **`FourierEncoder`**:\n",
    "    *   **Role**: Encodes the continuous scalar timestep `t` into a higher-dimensional embedding vector. This allows the network to effectively condition its behavior on the current noise level/timestep.\n",
    "    *   **Input**: A tensor `t` of shape `(bs, 1, 1, 1)` containing timestep values (typically normalized between 0 and 1).\n",
    "    *   **Output**: A tensor `embeddings` of shape `(bs, dim)`, where `dim` is the specified embedding dimension.\n",
    "    *   **Process**: It uses a learnable set of frequencies (`self.weights`) to project the input timestep `t` onto multiple sine and cosine waves of different frequencies. The resulting sine and cosine values are concatenated to form the final embedding. This is similar to positional encoding but uses learnable frequencies.\n",
    "\n",
    "2.  **`ResidualLayer`**:\n",
    "    *   **Role**: Forms the core building block of the U-Net's encoder, decoder, and mid-section. It processes the spatial feature map while incorporating conditional information (time and class label embeddings). The residual connection helps with gradient flow and allows the network to learn modifications to the identity mapping.\n",
    "    *   **Input**:\n",
    "        *   `x`: Feature map tensor of shape `(bs, c, h, w)`.\n",
    "        *   `t_embed`: Time embedding tensor of shape `(bs, t_embed_dim)`.\n",
    "        *   `y_embed`: Class label embedding tensor of shape `(bs, y_embed_dim)`.\n",
    "    *   **Output**: Feature map tensor of the same shape as input `x`, `(bs, c, h, w)`.\n",
    "    *   **Process**: It first passes the input `x` through a convolutional block (`block1`). Then, it adapts the time (`t_embed`) and class (`y_embed`) embeddings to match the number of channels `c` using linear layers (`time_adapter`, `y_adapter`) and adds them element-wise to the feature map. The result is passed through a second convolutional block (`block2`). Finally, the original input `x` (stored as `res`) is added back (residual connection) to the output of `block2`. Group Normalization and SiLU activations are used within the blocks.\n",
    "\n",
    "3.  **`Encoder`**:\n",
    "    *   **Role**: Part of the downsampling path (contracting path) of the U-Net. It extracts increasingly abstract spatial features while reducing the spatial resolution (height and width) and increasing the number of channels.\n",
    "    *   **Input**:\n",
    "        *   `x`: Feature map tensor of shape `(bs, c_in, h, w)`.\n",
    "        *   `t_embed`: Time embedding tensor of shape `(bs, t_embed_dim)`.\n",
    "        *   `y_embed`: Class label embedding tensor of shape `(bs, y_embed_dim)`.\n",
    "    *   **Output**: Feature map tensor of shape `(bs, c_out, h // 2, w // 2)`.\n",
    "    *   **Process**: It consists of multiple `ResidualLayer`s operating at the input resolution (`c_in`, `h`, `w`), followed by a downsampling convolutional layer (`nn.Conv2d` with `stride=2`) that halves the height and width and adjusts the number of channels from `c_in` to `c_out`.\n",
    "\n",
    "4.  **`Midcoder`**:\n",
    "    *   **Role**: Represents the bottleneck or the lowest-resolution part of the U-Net, connecting the encoder (downsampling path) and the decoder (upsampling path). It further processes the features at the smallest spatial dimension.\n",
    "    *   **Input**:\n",
    "        *   `x`: Feature map tensor of shape `(bs, c, h, w)` (where `c` is the maximum number of channels, and `h`, `w` are the minimum spatial dimensions).\n",
    "        *   `t_embed`: Time embedding tensor of shape `(bs, t_embed_dim)`.\n",
    "        *   `y_embed`: Class label embedding tensor of shape `(bs, y_embed_dim)`.\n",
    "    *   **Output**: Feature map tensor of the same shape as input `x`, `(bs, c, h, w)`.\n",
    "    *   **Process**: It simply consists of one or more `ResidualLayer`s that operate on the input feature map without changing its spatial dimensions or channel count.\n",
    "\n",
    "5.  **`Decoder`**:\n",
    "    *   **Role**: Part of the upsampling path (expansive path) of the U-Net. It gradually increases the spatial resolution while decreasing the number of channels, aiming to reconstruct the output image (or noise prediction) at the original resolution. It typically incorporates skip connections from the corresponding encoder stage.\n",
    "    *   **Input**:\n",
    "        *   `x`: Feature map tensor of shape `(bs, c_in, h, w)`.\n",
    "        *   `t_embed`: Time embedding tensor of shape `(bs, t_embed_dim)`.\n",
    "        *   `y_embed`: Class label embedding tensor of shape `(bs, y_embed_dim)`.\n",
    "    *   **Output**: Feature map tensor of shape `(bs, c_out, 2 * h, 2 * w)`.\n",
    "    *   **Process**: It first applies an upsampling operation (here, bilinear interpolation followed by a convolution) to double the height and width and adjust channels from `c_in` to `c_out`. Then, it passes the result through multiple `ResidualLayer`s operating at the new, larger resolution. *Note: The skip connection addition happens outside the Decoder block, before passing `x` to the decoder in the main U-Net forward pass.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab990aea",
   "metadata": {
    "id": "V4XtXQBhfxFV"
   },
   "source": [
    "## 3.2 Classifier-Free Guidance\n",
    "\n",
    "Implement CFG requires a small modification to the diffusion training and sampling code.\n",
    "\n",
    "*During training*, we randomly drop out the class label with a certain probability `drop_prob=0.1`, i.e. we use a dummy embedding to replace the digit label embedding.\n",
    "\n",
    "*During sampling*, given a digit label, instead of using $\\hat{\\epsilon} = f_\\theta(x_t, t, y)$ to sample, use:\n",
    "$$\\hat{\\epsilon} = f_\\theta(x_t, t, \\varnothing) + w(f_\\theta(x_t, t, y) - f_\\theta(x_t, t, \\varnothing))$$\n",
    "where $w$ is a sampling hyperparameter that controls the strength of CFG. $\\varnothing$ indicates the unconditional model with the class label dropped out, which is supported by the dummy embedding during training. Note that $w = 1$ recovers standard sampling.\n",
    "\n",
    "Please modify the code you wrote for Problem 2 for unconditional generation to adapt for the CFG setting. You can change the model architecture from MLP to UNet. Specifically, given a data element $x$ and U-Net $f_\\theta(x, t)$, implement the following diffusion training steps similar as what we did in Problem 2, while using a different noise schedule (in step 2):\n",
    "0. Construct a class `MNISTDiffusion`\n",
    "1. Sample the diffusion timestep: $t \\sim \\text{Uniform}(0, 1)$\n",
    "2. Compute the noise-strength following a linear schedule: $\\alpha_t = 1-t, \\sigma_t = \\sqrt{1-(1-t)^2}$\n",
    "3. Sample noise $\\epsilon \\sim N(0,I)$ (same shape as $x$) and cmpute noised $x_t = \\alpha_t x + \\sigma_t \\epsilon$\n",
    "4. Estimate $\\hat{\\epsilon} = f_\\theta(x_t, t)$\n",
    "5. Optimize the loss $L = \\lVert \\epsilon - \\hat{\\epsilon} \\rVert_2^2$. Here, it suffices to just take the mean over all dimensions.\n",
    "\n",
    "*Note*: you can reuse your code from Problem 2 for functions `train`, `eval_loss`, `get_lr`, and `train_epochs`.\n",
    "\n",
    "*Hyperparameter details*\n",
    "* UNet with hidden_dims as [64, 128] and 1 blocks_per_dim\n",
    "* Train 10 epochs, batch size 128, Adam with LR 1e-3 (0 warmup steps, and `use_cosine_decay=True`)\n",
    "* Training 10 epochs takes about 7 minutes on the Google Colab T4 GPU.\n",
    "\n",
    "After training, please generate 4 images for each of the 10 digit labels using guidance strength $w$ of 0.0, 0.5, 1.0, 2.0, 4.0 respectively, and save the images as 4x10 grid of images (each column is a digit). You are required to submit the images for each guidance strength along with the training loss curve. **Comparing the results with different $w$, what can you say about its impact to the generation performance?**\n",
    "\n",
    "*Hint*: To check your answer, the final test loss is below 0.02."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b141c3",
   "metadata": {
    "id": "short_answer_cfg"
   },
   "source": [
    "**Answer (CFG Impact):**\n",
    "\n",
    "Comparing the generated images across different values of the classifier-free guidance weight `w`:\n",
    "*   **`w = 0.0`**: This corresponds to purely unconditional generation (using only $f_\\theta(x_t, t, \\varnothing)$). The samples might resemble MNIST digits in general structure but likely won't correspond clearly to any specific digit class. The diversity might be high, but the adherence to specific labels is non-existent.\n",
    "*   **`w = 0.5` or `w = 1.0`**: These represent mild to standard guidance. As `w` increases from 0, the samples should start looking more distinctly like the target digits for each column. `w = 1.0` recovers the standard conditional prediction $f_\\theta(x_t, t, y)$. The samples should be recognizable digits corresponding to their intended class labels.\n",
    "*   **`w = 2.0` or `w = 4.0`**: These represent stronger guidance. Increasing `w` further pushes the generation process more strongly towards the conditional prediction $f_\\theta(x_t, t, y)$ relative to the unconditional one $f_\\theta(x_t, t, \\varnothing)$. This often leads to:\n",
    "    *   **Increased Sample Quality/Clarity**: The generated digits usually become sharper, clearer, and more prototypical examples of their respective classes. The features distinguishing the digits (like the loop in '6' or the sharp corners in '7') become more pronounced.\n",
    "    *   **Decreased Diversity**: While the quality might increase, the diversity within each class might decrease. The model generates samples that are very \"confident\" examples of the class, potentially losing some of the variation seen in the real dataset. At very high `w`, samples might start looking somewhat artificial or exaggerated.\n",
    "\n",
    "In summary, the CFG weight `w` controls a trade-off between sample quality/adherence to the condition (class label) and sample diversity. Higher `w` improves quality and class consistency but reduces diversity, while lower `w` increases diversity at the cost of potentially lower quality and weaker class conditioning. `w=0` removes conditioning entirely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf00134",
   "metadata": {
    "id": "KeB_8PeiffOU"
   },
   "outputs": [],
   "source": [
    "class MNISTDiffusion:\n",
    "    def __init__(self, model, data_shape, device, n_classes, drop_prob=0.1):\n",
    "        \"\"\"\n",
    "        model: neural network to estimate eps_hat (U-Net in this problem)\n",
    "        data_shape: size of the input data, (1, 32, 32) after padding\n",
    "        device: cuda or cpu\n",
    "        n_classes: number of classes for conditional generation, set to 10 in this problem\n",
    "        drop_prob: probability of dropping the condition in CFG training\n",
    "        \"\"\"\n",
    "        self.model = model.to(device)\n",
    "        self.data_shape = data_shape\n",
    "        self.device = device\n",
    "        self.drop_prob = drop_prob\n",
    "        self.n_classes = n_classes\n",
    "        # Use n_classes as the embedding index for the unconditional/dropped class\n",
    "        self.uncond_label = n_classes\n",
    "\n",
    "    def loss(self, x, y):\n",
    "        \"\"\"\n",
    "        x: the input data (batch_size, 1, 32, 32), normalized and padded\n",
    "        y: the class label (batch_size,)\n",
    "        Return:\n",
    "            The loss (as a scalar averaged over all data in the batch)\n",
    "        \"\"\"\n",
    "        # 1. Sample t ~ Uniform(0, 1)\n",
    "        # Use a small epsilon to avoid t=0 or t=1 exactly, where schedule might be ill-defined\n",
    "        t = torch.rand(x.shape[0], device=self.device) * (1.0 - 1e-4) + 1e-4\n",
    "\n",
    "        # 2. Compute alpha_t, sigma_t using the LINEAR schedule\n",
    "        alpha_t, sigma_t = alpha_sigma_linear_schedule(t)\n",
    "        alpha_t = alpha_t.view(-1, 1, 1, 1) # Reshape for broadcasting (bs, 1, 1, 1)\n",
    "        sigma_t = sigma_t.view(-1, 1, 1, 1)\n",
    "\n",
    "        # 3. Sample epsilon ~ N(0, I) and compute x_t\n",
    "        epsilon = torch.randn_like(x, device=self.device)\n",
    "        x_t = alpha_t * x + sigma_t * epsilon\n",
    "\n",
    "        # 4. Classifier-Free Guidance: Randomly drop condition\n",
    "        context_mask = torch.bernoulli(torch.ones(x.shape[0], device=self.device) * (1 - self.drop_prob))\n",
    "        # Set dropped labels to the unconditional label index\n",
    "        y_masked = y * context_mask.long() + self.uncond_label * (1 - context_mask.long())\n",
    "\n",
    "        # 5. Estimate eps_hat using the U-Net model\n",
    "        # U-Net expects t in shape (bs, 1, 1, 1)\n",
    "        t_unet = t.view(-1, 1, 1, 1)\n",
    "        eps_hat = self.model(x_t, t_unet, y_masked)\n",
    "\n",
    "        # 6. Compute loss L = ||epsilon - eps_hat||^2 (mean over batch and dims)\n",
    "        loss = torch.mean((epsilon - eps_hat)**2)\n",
    "        return loss\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sample(self, n, num_steps, guide_w=0.0):\n",
    "        \"\"\"\n",
    "        n: number of samples to generate (generate n // n_classes samples for each class)\n",
    "        num_steps: number of steps in the diffusion sampling\n",
    "        guide_w: the CFG weight\n",
    "        Return:\n",
    "            The generated sample. Tensor with shape (n, *self.data_shape)\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        samples_per_class = n // self.n_classes\n",
    "\n",
    "        # Labels for conditional generation (samples_per_class of each digit 0-9)\n",
    "        y_cond = torch.arange(self.n_classes, device=self.device).repeat_interleave(samples_per_class)\n",
    "        # Unconditional labels (all set to uncond_label index)\n",
    "        y_uncond = torch.full_like(y_cond, self.uncond_label)\n",
    "\n",
    "        # Define timesteps (from 1-eps down to eps)\n",
    "        ts = torch.linspace(1.0 - 1e-4, 1e-4, num_steps + 1, device=self.device)\n",
    "\n",
    "        # Start with x1 ~ N(0, I)\n",
    "        x = torch.randn(n, *self.data_shape, device=self.device)\n",
    "\n",
    "        # DDPM sampling loop\n",
    "        for i in tqdm(range(num_steps), desc=f\"Sampling w={guide_w}\", leave=False):\n",
    "            t_ = ts[i]\n",
    "            tm1_ = ts[i+1]\n",
    "\n",
    "            # Prepare t and tm1 tensors (scalar repeated for batch)\n",
    "            t = torch.full((n,), t_, device=self.device)\n",
    "            tm1 = torch.full((n,), tm1_, device=self.device)\n",
    "\n",
    "            # Get alpha and sigma for t and t-1 using LINEAR schedule\n",
    "            alpha_t, sigma_t = alpha_sigma_linear_schedule(t)\n",
    "            alpha_tm1, sigma_tm1 = alpha_sigma_linear_schedule(tm1)\n",
    "\n",
    "            # Reshape for broadcasting\n",
    "            alpha_t = alpha_t.view(-1, 1, 1, 1)\n",
    "            sigma_t = sigma_t.view(-1, 1, 1, 1)\n",
    "            alpha_tm1 = alpha_tm1.view(-1, 1, 1, 1)\n",
    "            sigma_tm1 = sigma_tm1.view(-1, 1, 1, 1)\n",
    "\n",
    "            # Predict epsilon_hat using CFG\n",
    "            t_unet = t.view(-1, 1, 1, 1) # U-Net expects (bs, 1, 1, 1)\n",
    "            eps_hat_cond = self.model(x, t_unet, y_cond)\n",
    "            eps_hat_uncond = self.model(x, t_unet, y_uncond)\n",
    "            eps_hat = eps_hat_uncond + guide_w * (eps_hat_cond - eps_hat_uncond)\n",
    "\n",
    "            # Calculate eta_t for DDPM (using linear schedule components)\n",
    "            ratio_alpha_sq = (alpha_t / alpha_tm1)**2\n",
    "            ratio_alpha_sq = torch.clamp(ratio_alpha_sq, max=1.0 - 1e-6) # Avoid issues near t=1\n",
    "            # Ensure sigma_t is not zero before division\n",
    "            sigma_t_safe = torch.clamp(sigma_t, min=1e-8)\n",
    "            eta_t_sq = (sigma_tm1 / sigma_t_safe)**2 * (1 - ratio_alpha_sq)\n",
    "            eta_t = torch.sqrt(torch.clamp(eta_t_sq, min=1e-8))\n",
    "\n",
    "            # Calculate coefficients for the DDPM update step (from Problem 2, adapted)\n",
    "            term1 = alpha_tm1 * ((x - sigma_t * eps_hat) / alpha_t)\n",
    "\n",
    "            sigma_tm1_sq_minus_eta_t_sq = sigma_tm1**2 - (sigma_tm1**2 / sigma_t_safe**2) * (1 - ratio_alpha_sq)\n",
    "            sigma_tm1_sq_minus_eta_t_sq_clipped = torch.clamp(sigma_tm1_sq_minus_eta_t_sq, min=0.0)\n",
    "            term2_coeff = torch.sqrt(sigma_tm1_sq_minus_eta_t_sq_clipped)\n",
    "            term2 = term2_coeff * eps_hat\n",
    "\n",
    "            # Sample random noise eps_t ~ N(0, I) only if not the last step\n",
    "            term3 = 0.0\n",
    "            if tm1_ > 1e-4: # Add noise except for the very last step\n",
    "                eps_t = torch.randn_like(x, device=self.device)\n",
    "                term3 = eta_t * eps_t\n",
    "\n",
    "            # DDPM Update\n",
    "            x = term1 + term2 + term3\n",
    "\n",
    "        self.model.train() # Switch back to train mode after sampling\n",
    "        # Denormalize samples? No, keep them normalized. Plotting function handles denormalization if needed.\n",
    "        # Clamp samples to [-1, 1] range typical for normalized images\n",
    "        x = torch.clamp(x, -1.0, 1.0)\n",
    "        return x\n",
    "\n",
    "    def __getattr__(self, name):\n",
    "        if name in ['train', 'eval', 'parameters', 'state_dict', 'load_state_dict']:\n",
    "            return getattr(self.model, name)\n",
    "        return self.__getattribute__(name)\n",
    "\n",
    "\n",
    "# --- Reusable Training Functions (Adapted from P2) ---\n",
    "\n",
    "def train(model, train_loader, optimizer, scheduler):\n",
    "    \"\"\"\n",
    "    model: model to train, the MNISTDiffusion class in this case.\n",
    "    train_loader: train_loader for MNIST (yields image, label)\n",
    "    optimizer: use torch.optim.Adam\n",
    "    scheduler: use optim.lr_scheduler.LambdaLR\n",
    "    Return:\n",
    "        Tensor with train loss of each batch\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    device = model.device # Get device from model\n",
    "    for batch in tqdm(train_loader, desc=\"Training Batch\", leave=False):\n",
    "        x, y = batch # Unpack image and label\n",
    "        x = x.to(device) # Move data to device\n",
    "        y = y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = model.loss(x, y) # Pass both x and y to loss function\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step() # Step the scheduler each batch\n",
    "\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "    return torch.tensor(train_losses)\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_loss(model, data_loader):\n",
    "    \"\"\"\n",
    "    model: model to train, the MNISTDiffusion class in this case.\n",
    "    data_loader: test_loader for MNIST (yields image, label)\n",
    "    Return:\n",
    "        Scalar with the average test loss over the dataset\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    count = 0\n",
    "    device = model.device\n",
    "    for batch in data_loader:\n",
    "        x, y = batch\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        loss = model.loss(x, y) # Pass both x and y\n",
    "        total_loss += loss.item() * x.shape[0]\n",
    "        count += x.shape[0]\n",
    "    return total_loss / count if count > 0 else 0.0\n",
    "\n",
    "def get_lr(step, total_steps, warmup_steps, lr_init, use_cos_decay):\n",
    "    \"\"\"\n",
    "    Function that returns the learning rate for the specific step.\n",
    "    Handles linear warmup and optional cosine decay.\n",
    "    (Identical to P2 version)\n",
    "    \"\"\"\n",
    "    if warmup_steps > 0 and step < warmup_steps:\n",
    "        # Linear warmup\n",
    "        lr = lr_init * (step + 1) / warmup_steps\n",
    "    elif use_cos_decay:\n",
    "        # Cosine decay after warmup\n",
    "        # Ensure total_steps > warmup_steps for calculation\n",
    "        effective_total_steps = max(1, total_steps - warmup_steps)\n",
    "        progress = (step - warmup_steps) / effective_total_steps\n",
    "        lr = lr_init * 0.5 * (1.0 + math.cos(math.pi * progress))\n",
    "    else:\n",
    "        # Constant learning rate (or constant after warmup if warmup_steps > 0)\n",
    "        lr = lr_init\n",
    "\n",
    "    # Prevent negative LR if progress calculation leads to it (shouldn't happen with max(1,...))\n",
    "    lr = max(lr, 0.0)\n",
    "    return lr\n",
    "\n",
    "def train_epochs(model, train_loader, test_loader, train_args):\n",
    "    \"\"\"\n",
    "    model: model to train, the MNISTDiffusion class in this case.\n",
    "    train_loader: train_loader for MNIST\n",
    "    test_loader: test_loader for MNIST\n",
    "    Return:\n",
    "        Two np.array for all the train losses (per batch) and test losses (per epoch)\n",
    "    \"\"\"\n",
    "    epochs, lr_init = train_args['epochs'], train_args['lr']\n",
    "    warmup_steps = train_args.get('warmup', 0)\n",
    "    use_cos_decay = train_args.get('use_cos_decay', False)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr_init)\n",
    "\n",
    "    # Calculate total steps ensuring train_loader is not empty\n",
    "    if len(train_loader) == 0:\n",
    "        print(\"Warning: train_loader is empty, cannot train.\")\n",
    "        return np.array([]), np.array([])\n",
    "    total_steps = epochs * len(train_loader)\n",
    "\n",
    "    # Define scheduler using get_lr\n",
    "    scheduler = optim.lr_scheduler.LambdaLR(\n",
    "        optimizer,\n",
    "        lr_lambda=lambda step: get_lr(step, total_steps, warmup_steps, lr_init, use_cos_decay)\n",
    "    )\n",
    "\n",
    "    all_train_losses = []\n",
    "    all_test_losses = []\n",
    "\n",
    "    # Initial test loss\n",
    "    initial_test_loss = eval_loss(model, test_loader)\n",
    "    all_test_losses.append(initial_test_loss)\n",
    "    print(f\"Epoch 0 (Initial), Test Loss: {initial_test_loss:.4f}\")\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        epoch_train_losses = train(model, train_loader, optimizer, scheduler)\n",
    "        # Ensure losses are moved to CPU before extending list if they aren't already\n",
    "        all_train_losses.extend(epoch_train_losses.cpu().numpy())\n",
    "\n",
    "        epoch_test_loss = eval_loss(model, test_loader)\n",
    "        all_test_losses.append(epoch_test_loss)\n",
    "\n",
    "        # Get last learning rate, handle potential list wrapping\n",
    "        current_lr = scheduler.get_last_lr()[0] if scheduler.get_last_lr() else lr_init\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Test Loss: {epoch_test_loss:.4f}, LR: {current_lr:.6f}\")\n",
    "\n",
    "    return np.array(all_train_losses), np.array(all_test_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce6ae5c",
   "metadata": {
    "id": "qpsSdUPvffKx"
   },
   "outputs": [],
   "source": [
    "def mnist_diffusion(train_loader, test_loader):\n",
    "    \"\"\"\n",
    "    train_loader: MNIST training data loader (original 28x28)\n",
    "    test_loader: MNIST test data loader (original 28x28)\n",
    "\n",
    "    Returns\n",
    "    - a (# of training iterations,) numpy array of train losses evaluated every minibatch\n",
    "    - a (# of num_epochs + 1,) numpy array of test losses evaluated at the start of training and the end of every epoch\n",
    "\n",
    "    Generates and saves 4 images for each of the 10 digits in `./results/`,\n",
    "    using guidance strengths of 0.0, 0.5, 1.0, 2.0, 4.0.\n",
    "    Saves the images as `./results/image_w{w}.png` as 4x10 grid (each column is a digit)\n",
    "    hint: x_gen is the output from model.sample, use the following to generate and save grids\n",
    "            grid = make_grid(x_gen*-1 + 1, nrow=10) # Note: MNIST data is 0-1, model output is likely -1 to 1.\n",
    "                                                     # Let's adjust the grid making: (x_gen + 1) / 2 to map [-1, 1] -> [0, 1]\n",
    "            save_image(grid, save_dir + f\"image_w{w}.png\")\n",
    "    \"\"\"\n",
    "\n",
    "    # --- Hyperparameters & Setup ---\n",
    "    n_classes = 10\n",
    "    save_dir = './results/'\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    ws_test = [0.0, 0.5, 1.0, 2.0, 4.0] # strength of generative guidance\n",
    "    num_steps = 512 # Sampling steps\n",
    "    num_samples_per_run = 40 # Generate 4 samples per class (4 * 10 = 40)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Training args\n",
    "    epochs = 10\n",
    "    lr = 1e-3\n",
    "    batch_size = 128 # Already defined by input loaders, but good to note\n",
    "    warmup_steps = 0\n",
    "    use_cos_decay = True\n",
    "\n",
    "    # --- Data Preprocessing --- #\n",
    "    # Pad MNIST from 28x28 to 32x32 and normalize to [-1, 1]\n",
    "    # U-Net expects powers of 2 for dimensions usually\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Pad(2), # Pad 2 pixels on each side (28+2*2 = 32)\n",
    "        transforms.ToTensor(), # Converts to [0, 1]\n",
    "        transforms.Normalize((0.5,), (0.5,)) # Normalize to [-1, 1]\n",
    "    ])\n",
    "\n",
    "    # Recreate datasets and loaders with the new transform\n",
    "    train_dataset_processed = MNIST(\"./data\", train=True, download=False, transform=transform) # Already downloaded\n",
    "    test_dataset_processed = MNIST(\"./data\", train=False, download=False, transform=transform)\n",
    "\n",
    "    train_loader_processed = DataLoader(train_dataset_processed, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
    "    test_loader_processed = DataLoader(test_dataset_processed, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "    # --- Model Initialization ---\n",
    "    unet_model = MNISTUNet(\n",
    "        channels = [64, 128], # As per instructions\n",
    "        num_residual_layers = 1, # As per instructions\n",
    "        t_embed_dim = 64, # Choose a suitable dimension\n",
    "        y_embed_dim = 64, # Choose a suitable dimension\n",
    "    )\n",
    "    # Data shape is (1, 32, 32) after padding\n",
    "    diffusion_model = MNISTDiffusion(unet_model, (1, 32, 32), device=device, n_classes=n_classes, drop_prob=0.1)\n",
    "\n",
    "    # --- Training ---\n",
    "    train_args = {\n",
    "        'epochs': epochs,\n",
    "        'lr': lr,\n",
    "        'warmup': warmup_steps,\n",
    "        'use_cos_decay': use_cos_decay\n",
    "    }\n",
    "\n",
    "    print(f\"Starting MNIST training on {device}...\")\n",
    "    all_train_losses, all_test_losses = train_epochs(\n",
    "        diffusion_model,\n",
    "        train_loader_processed, # Use processed loader\n",
    "        test_loader_processed,  # Use processed loader\n",
    "        train_args\n",
    "    )\n",
    "    print(\"MNIST training finished.\")\n",
    "\n",
    "    # --- Sampling & Saving --- #\n",
    "    print(\"Generating and saving samples for different guidance weights...\")\n",
    "    diffusion_model.eval() # Set model to evaluation mode\n",
    "\n",
    "    for w in ws_test:\n",
    "        # Generate 4 samples per class\n",
    "        x_gen_normalized = diffusion_model.sample(n=num_samples_per_run, num_steps=num_steps, guide_w=w)\n",
    "\n",
    "        # Map samples from [-1, 1] back to [0, 1] for saving\n",
    "        x_gen_0_1 = (x_gen_normalized + 1) / 2.0\n",
    "\n",
    "        # Create a grid (4 rows, 10 columns)\n",
    "        # make_grid expects (N, C, H, W)\n",
    "        grid = make_grid(x_gen_0_1, nrow=n_classes) # nrow=10 to have each column be a class\n",
    "\n",
    "        # Save the image\n",
    "        save_image(grid, os.path.join(save_dir, f\"image_w{w}.png\"))\n",
    "        print(f\"Saved image grid to {os.path.join(save_dir, f'image_w{w}.png')}\")\n",
    "\n",
    "    # Return training and test losses\n",
    "    return all_train_losses, all_test_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ffb256",
   "metadata": {
    "id": "bTfqxUM8Exb9"
   },
   "outputs": [],
   "source": [
    "def save_training_plot(\n",
    "    train_losses: np.ndarray, test_losses: np.ndarray, title: str, fname: str\n",
    ") -> None:\n",
    "    plt.figure()\n",
    "    n_epochs = len(test_losses) - 1\n",
    "    x_train = np.linspace(0, n_epochs, len(train_losses))\n",
    "    x_test = np.arange(n_epochs + 1)\n",
    "\n",
    "    plt.plot(x_train, train_losses, label=\"train loss\")\n",
    "    plt.plot(x_test, test_losses, label=\"test loss\")\n",
    "    plt.legend()\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"NLL\")\n",
    "    savefig(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6748e56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses, test_losses = mnist_diffusion(train_loader, test_loader)\n",
    "print(f\"Final Test Loss: {test_losses[-1]:.4f}\")\n",
    "\n",
    "save_training_plot(\n",
    "    train_losses,\n",
    "    test_losses,\n",
    "    f\"MNIST Train Plot\",\n",
    "    f\"results/mnist_train_plot.png\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a49979",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.image as mpimg\n",
    "\n",
    "ws_test = [0.0, 0.5, 1.0, 2.0, 4.0]\n",
    "for w in ws_test:\n",
    "  img_path = f'results/image_w{w}.png'\n",
    "  img = mpimg.imread(img_path)\n",
    "  plt.title(f'w={w}')\n",
    "  plt.imshow(img)\n",
    "  plt.axis('off')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf14a49",
   "metadata": {
    "id": "2qlmbEblExb-"
   },
   "source": [
    "# Submission Guideline for Part 2\n",
    "\n",
    "Please include your answer to all problems, including formulas, proofs, and the figures generated in each problem, excluding code. You are required to submit the (single) pdf and all (four) notebooks (one for each problem) with your code and running outputs. Do not include code in the pdf file.\n",
    "\n",
    "Specifically, for Problem 3 in this notebook, the pdf should contain:\n",
    "- The generated figures `results/mnist_train_plot.png` and `results/image_w{w}.png` (w=0.0, 0.5, 1.0, 2.0, 4.0)\n",
    "- Answer to the short answer question about the U-Net architecture\n",
    "- Answer to the short answer question about different CFG weight $w$ in problem 3.2"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "main_language": "python"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
